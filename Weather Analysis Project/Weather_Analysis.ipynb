{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jsb5KLkmVyFJ",
        "outputId": "f0f7b99c-18dd-48f4-a3f4-d0c62c960dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Date     AlfaX     AlfaY     AlfaZ     AlfaD    BravoX    BravoY  \\\n",
            "0  1874-Mar-19 -0.387448 -0.004925  0.035286  0.389083  0.702318  0.178241   \n",
            "1  1874-Mar-20 -0.391974 -0.031792  0.033512  0.394686  0.696998  0.197682   \n",
            "2  1874-Mar-21 -0.394613 -0.058506  0.031577  0.400175  0.691137  0.216970   \n",
            "3  1874-Mar-22 -0.395431 -0.084952  0.029497  0.405527  0.684740  0.236090   \n",
            "4  1874-Mar-23 -0.394494 -0.111021  0.027285  0.410726  0.677812  0.255027   \n",
            "\n",
            "     BravoZ    BravoD  CharlieX  ...      PapaY     PapaZ      PapaD  \\\n",
            "0 -0.038235  0.725591 -0.993904  ...  13.967170  0.209719  17.842542   \n",
            "1 -0.037669  0.725467 -0.994185  ...  13.981678  0.209753  17.855734   \n",
            "2 -0.037073  0.725342 -0.994298  ...  13.996177  0.209787  17.869108   \n",
            "3 -0.036448  0.725215 -0.994218  ...  14.010663  0.209821  17.882659   \n",
            "4 -0.035795  0.725086 -0.993915  ...  14.025129  0.209855  17.896382   \n",
            "\n",
            "     QuebecX    QuebecY   QuebecZ    QuebecD  7P  14P  28P  \n",
            "0  26.853693  14.828570 -0.900904  30.689069   1    5    5  \n",
            "1  26.852167  14.848579 -0.900919  30.697408   2    5    5  \n",
            "2  26.850342  14.868580 -0.900934  30.705491   2    5    5  \n",
            "3  26.848217  14.888568 -0.900948  30.713318   2    5    5  \n",
            "4  26.845795  14.908537 -0.900963  30.720887   2    5    5  \n",
            "\n",
            "[5 rows x 72 columns]\n",
            "Missing Values:\n",
            " Date       0\n",
            "AlfaX      0\n",
            "AlfaY      0\n",
            "AlfaZ      0\n",
            "AlfaD      0\n",
            "          ..\n",
            "QuebecZ    0\n",
            "QuebecD    0\n",
            "7P         0\n",
            "14P        0\n",
            "28P        0\n",
            "Length: 72, dtype: int64\n",
            "Statistical Summary:\n",
            "               AlfaX         AlfaY         AlfaZ         AlfaD        BravoX  \\\n",
            "count  54544.000000  54544.000000  54544.000000  54544.000000  54544.000000   \n",
            "mean      -0.026440     -0.116215     -0.007055      0.395282      0.004303   \n",
            "std        0.266908      0.269782      0.032768      0.055684      0.510549   \n",
            "min       -0.395431     -0.462967     -0.051166      0.307491     -0.718902   \n",
            "25%       -0.293054     -0.381542     -0.039671      0.341002     -0.506171   \n",
            "50%       -0.035224     -0.152953     -0.009378      0.403024      0.005279   \n",
            "75%        0.240193      0.147097      0.025527      0.450003      0.514864   \n",
            "max        0.360194      0.308003      0.041805      0.466704      0.725413   \n",
            "\n",
            "             BravoY        BravoZ        BravoD      CharlieX      CharlieY  \\\n",
            "count  54544.000000  54544.000000  54544.000000  54544.000000  54544.000000   \n",
            "mean      -0.005144     -0.000318      0.723344      0.004569     -0.026021   \n",
            "std        0.511494      0.030282      0.003475      0.706872      0.707192   \n",
            "min       -0.726988     -0.043083      0.718364     -0.998815     -1.018999   \n",
            "25%       -0.516716     -0.030589      0.719870     -0.701954     -0.733224   \n",
            "50%       -0.006473     -0.000395      0.723357      0.005196     -0.036056   \n",
            "75%        0.506358      0.029964      0.726817      0.711186      0.681193   \n",
            "max        0.719617      0.042607      0.728295      1.006268      0.986330   \n",
            "\n",
            "       ...         PapaY         PapaZ         PapaD       QuebecX  \\\n",
            "count  ...  54544.000000  54544.000000  54544.000000  54544.000000   \n",
            "mean   ...     -2.406523     -0.027978     19.246900     -3.274375   \n",
            "std    ...     12.767387      0.189038      0.966005     20.081111   \n",
            "min    ...    -20.296179     -0.271500     17.287770    -31.246711   \n",
            "25%    ...    -14.785888     -0.215351     18.581197    -22.981312   \n",
            "50%    ...     -3.642613     -0.054572     19.234567     -4.816800   \n",
            "75%    ...      9.339483      0.170450     19.966032     15.781669   \n",
            "max    ...     20.038717      0.247575     21.092789     30.779014   \n",
            "\n",
            "            QuebecY       QuebecZ       QuebecD            7P           14P  \\\n",
            "count  54544.000000  54544.000000  54544.000000  54544.000000  54544.000000   \n",
            "mean      -0.944294      0.095256     30.104614      2.855419      3.190268   \n",
            "std       22.171074      0.633376      0.725823      1.525008      1.314695   \n",
            "min      -31.238494     -0.921142     28.821933      1.000000      1.000000   \n",
            "25%      -22.984530     -0.483295     29.418836      1.000000      2.000000   \n",
            "50%       -4.803903      0.151976     30.112840      3.000000      3.000000   \n",
            "75%       22.480204      0.714648     30.783905      4.000000      4.000000   \n",
            "max       30.890146      0.936543     31.324430      5.000000      5.000000   \n",
            "\n",
            "                28P  \n",
            "count  54544.000000  \n",
            "mean       3.553626  \n",
            "std        1.209900  \n",
            "min        1.000000  \n",
            "25%        2.000000  \n",
            "50%        4.000000  \n",
            "75%        5.000000  \n",
            "max        5.000000  \n",
            "\n",
            "[8 rows x 71 columns]\n",
            "Target Distribution:\n",
            "       7P    14P    28P\n",
            "1  15065   5646   1059\n",
            "2  10684  14554  12858\n",
            "3   7925   9563  12601\n",
            "4   8812  13338  10879\n",
            "5  12058  11443  17147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-94b02ff9981d>:17: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  correlation_matrix = weather_data.corr()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation Matrix:\n",
            "              AlfaX     AlfaY     AlfaZ     AlfaD    BravoX    BravoY  \\\n",
            "AlfaX     1.000000  0.012079 -0.740589 -0.224509 -0.000480  0.000002   \n",
            "AlfaY     0.012079  1.000000  0.662963 -0.977112 -0.000368 -0.000791   \n",
            "AlfaZ    -0.740589  0.662963  1.000000 -0.488534  0.000113 -0.000534   \n",
            "AlfaD    -0.224509 -0.977112 -0.488534  1.000000  0.000460  0.000770   \n",
            "BravoX   -0.000480 -0.000368  0.000113  0.000460  1.000000  0.001362   \n",
            "BravoY    0.000002 -0.000791 -0.000534  0.000770  0.001362  1.000000   \n",
            "BravoZ    0.000467  0.000177 -0.000232 -0.000271 -0.973555  0.227117   \n",
            "BravoD   -0.000322  0.000343  0.000473 -0.000265  0.662736 -0.747929   \n",
            "CharlieX  0.000139  0.000769  0.000412 -0.000779 -0.000132 -0.003357   \n",
            "CharlieY -0.000267 -0.000421 -0.000084  0.000467  0.003198  0.000193   \n",
            "\n",
            "            BravoZ    BravoD  CharlieX  CharlieY  \n",
            "AlfaX     0.000467 -0.000322  0.000139 -0.000267  \n",
            "AlfaY     0.000177  0.000343  0.000769 -0.000421  \n",
            "AlfaZ    -0.000232  0.000473  0.000412 -0.000084  \n",
            "AlfaD    -0.000271 -0.000265 -0.000779  0.000467  \n",
            "BravoX   -0.973555  0.662736 -0.000132  0.003198  \n",
            "BravoY    0.227117 -0.747929 -0.003357  0.000193  \n",
            "BravoZ    1.000000 -0.816272 -0.000628 -0.003074  \n",
            "BravoD   -0.816272  1.000000  0.002445  0.002018  \n",
            "CharlieX -0.000628  0.002445  1.000000  0.000812  \n",
            "CharlieY -0.003074  0.002018  0.000812  1.000000  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/Inputs-Targets.csv'\n",
        "weather_data = pd.read_csv(file_path)\n",
        "\n",
        "print(weather_data.head())\n",
        "\n",
        "missing_values = weather_data.isnull().sum()\n",
        "print(\"Missing Values:\\n\", missing_values)\n",
        "\n",
        "statistical_summary = weather_data.describe()\n",
        "print(\"Statistical Summary:\\n\", statistical_summary)\n",
        "\n",
        "target_distribution = weather_data[['7P', '14P', '28P']].apply(pd.Series.value_counts)\n",
        "print(\"Target Distribution:\\n\", target_distribution)\n",
        "\n",
        "correlation_matrix = weather_data.corr()\n",
        "print(\"Correlation Matrix:\\n\", correlation_matrix.iloc[:10, :10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Explore Data - EDA\n",
        "\n",
        "1. Missing Values\n",
        "\n",
        "  No missing values were found, which makes the task a little easier to deal with <br>\n",
        "\n",
        "2. Distribution of target variables\n",
        "\n",
        "  The target variables are balanced in general but their distributions are different <br>\n",
        "3. Observing the statistical summary of the data\n",
        "  - The predictive features vary in scale and distribution, which calls for the need for scaling.\n",
        "  - The variety in scales also suggests that the features are related to geographic or atmospheric measurements.\n",
        "4. Looking for correlations between the features\n",
        "\n",
        "  The subset of the correlation matrix shows that the features have strong correlations with each other. The full matrix needs more analysis to understand all interactions.\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "yLD1BXahXUbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Deciding how to deal with multiple targets:\n",
        "\n",
        "There are multiple ways we can deal with multiple targets.\n",
        "1. Train Separate models for each target: this might give a higher accuracy since each model would be specialized for each target, however, it is time-consuming and resource-intensive.\n",
        "2. Tune one model using one target and apply it to the other targets: This approach is more reasonable since the targets are fairly similar. It would save time and resources.\n",
        "\n",
        "However, since the Extra credit requirements are asking to test each target separately, I will use multiple models to try predicting the rainfall."
      ],
      "metadata": {
        "id": "obQg8cL0i_3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Deciding which models to use\n",
        "\n",
        "7-day Rainfall: LSTM\n",
        "- Since 7 days is a short period, I am using the LSTM model (Long Short-Term Memory). These models are suitable for short dependencies in time-series data.\n",
        "\n",
        "14-day Rainfall: GRU\n",
        "- GRU is a simple and faster model to use on time-series data. It has fewer parameters and it is beneficial in predicting mid-range time series problems.\n",
        "\n",
        "28-day Rainfall: Random Forest (Machine Learning model)\n",
        "- Since it is a longer period, I will be using a random forest. The importance of specific sequential information is not as important in longer periods. A random forest is a good choice to capture complex non-linear relationships across multiple features.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s2-Wj7nvbmgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#First Model: LSTM\n",
        "\n",
        "To test this model, some steps need to be considered before training the model. <br>\n",
        "1. Data preprocessing: Scaling or normalizing, since neural networks are sensitive to the scale of the input data\n",
        "2. Preparing sequence: creating lag features and reshaping the data into a format suitable for LSTMs.\n",
        "3. Tuning the model: experimenting with different numbers of LSTM units, layers, and dropout rates."
      ],
      "metadata": {
        "id": "U9n9W8w9cgUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv('/content/Inputs-Targets.csv')\n",
        "\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data['Year'] = data['Date'].dt.year\n",
        "data['Month'] = data['Date'].dt.month\n",
        "\n",
        "X = data.drop(['Date', '7P', '14P', '28P'], axis=1)\n",
        "y = data['7P']\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_encoded = encoder.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
        "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
        "model.add(Dense(5, activation='softmax'))  # Output layer for 5 categories\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, validation_data=(X_test_reshaped, y_test), verbose=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWadfP-EUUIV",
        "outputId": "139b4f12-f298-4a58-a638-3b0fee079ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1364/1364 - 6s - loss: 1.5217 - accuracy: 0.3369 - val_loss: 1.5027 - val_accuracy: 0.3550 - 6s/epoch - 5ms/step\n",
            "Epoch 2/10\n",
            "1364/1364 - 5s - loss: 1.4794 - accuracy: 0.3658 - val_loss: 1.4753 - val_accuracy: 0.3675 - 5s/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "1364/1364 - 4s - loss: 1.4436 - accuracy: 0.3904 - val_loss: 1.4479 - val_accuracy: 0.3907 - 4s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "1364/1364 - 4s - loss: 1.4094 - accuracy: 0.4133 - val_loss: 1.4257 - val_accuracy: 0.4012 - 4s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "1364/1364 - 5s - loss: 1.3787 - accuracy: 0.4283 - val_loss: 1.4111 - val_accuracy: 0.4106 - 5s/epoch - 4ms/step\n",
            "Epoch 6/10\n",
            "1364/1364 - 4s - loss: 1.3524 - accuracy: 0.4463 - val_loss: 1.3885 - val_accuracy: 0.4289 - 4s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "1364/1364 - 4s - loss: 1.3284 - accuracy: 0.4596 - val_loss: 1.3766 - val_accuracy: 0.4296 - 4s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "1364/1364 - 4s - loss: 1.3068 - accuracy: 0.4695 - val_loss: 1.3634 - val_accuracy: 0.4384 - 4s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "1364/1364 - 4s - loss: 1.2874 - accuracy: 0.4797 - val_loss: 1.3513 - val_accuracy: 0.4433 - 4s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "1364/1364 - 4s - loss: 1.2702 - accuracy: 0.4873 - val_loss: 1.3413 - val_accuracy: 0.4532 - 4s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d0eccdae050>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train_reshaped shape:\", X_train_reshaped.shape)\n",
        "print(\"X_test_reshaped shape:\", X_test_reshaped.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_hKiUhLYQUK",
        "outputId": "b63f8b5b-f254-43a5-f437-8249434c77a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_reshaped shape: (43635, 1, 70)\n",
            "X_test_reshaped shape: (10909, 1, 70)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred = model.predict(X_test_reshaped)\n",
        "y_pred_classes = y_pred.argmax(axis=1)\n",
        "y_true = y_test.argmax(axis=1)\n",
        "\n",
        "print(confusion_matrix(y_true, y_pred_classes))\n",
        "\n",
        "print(classification_report(y_true, y_pred_classes))\n",
        "\n",
        "# # Plotting learning curves\n",
        "# plt.figure(figsize=(12, 4))\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.plot(model.history.history['accuracy'], label='Training Accuracy')\n",
        "# plt.plot(model.history.history['val_accuracy'], label='Validation Accuracy')\n",
        "# plt.title('Accuracy over Epochs')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.plot(model.history.history['loss'], label='Training Loss')\n",
        "# plt.plot(model.history.history['val_loss'], label='Validation Loss')\n",
        "# plt.title('Loss over Epochs')\n",
        "# plt.legend()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvqSwReyVhgQ",
        "outputId": "d29719db-48fc-461a-8b22-f3c71eb5e43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "341/341 [==============================] - 4s 5ms/step\n",
            "[[1991  358  146  131  365]\n",
            " [ 715  677  205  163  353]\n",
            " [ 385  255  339  222  334]\n",
            " [ 408  244  205  411  536]\n",
            " [ 384  172  155  229 1526]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.67      0.58      2991\n",
            "           1       0.40      0.32      0.35      2113\n",
            "           2       0.32      0.22      0.26      1535\n",
            "           3       0.36      0.23      0.28      1804\n",
            "           4       0.49      0.62      0.55      2466\n",
            "\n",
            "    accuracy                           0.45     10909\n",
            "   macro avg       0.42      0.41      0.40     10909\n",
            "weighted avg       0.43      0.45      0.43     10909\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Results:\n",
        "1. Accuracy: There are improvements in both training and validation accuracy over the epochs, which means that the model is learning correctly.\n",
        "2. Validation Accuracy: It is increasing which means that the model is succeeding in generalizing to unseen data. However, the maximum value for validation accuracy is 45%, which means that the model could be improved.\n",
        "3. Loss Reduction: the losses are decreasing, which is another indicator that the model is improving in performance. More training could take place since the values for the losses are still not low enough.\n",
        "\n",
        "# Interpretation of confusion matrix and Classification Report Analysis:\n",
        "- The confusion matrix suggests that the data might be imbalanced since the numbers across the rows and columns are not evenly distributed.\n",
        "- The classification report shows that categories 0 and 4 have the highest recall, which means that the model is better at identifying these categories. F1 score is also the highest for 0 and 4. The overall accuracy of 0.45 means that the model correctly predicts the category 45% of the time across all categories."
      ],
      "metadata": {
        "id": "40wiSmYJWjPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Next Steps:\n",
        "1. Increasing the number of epochs\n",
        "2. Tuning the hyperparameters of the model including LSTM units, learning rate, batch size, and dropout layers.\n",
        "3. Cross-validation"
      ],
      "metadata": {
        "id": "XegK6PyyXXfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code applied the following modifications: <br>\n",
        "1. Increases LSM units: it is increased to 100, which allows the model to capture more complex patterns\n",
        "2. Additional LSTM layer and dropout: This is helpful for regularization to prevent overfitting.\n",
        "3. Adjusted Learning rate: I set it to 0.001. This is a commonly used default value.\n",
        "4. Increased epochs and batch size: I increased both of them, which allows the model more opportunity to learn from the data and can help in achieving better convergence.\n",
        "5. Return sequence: I set this parameter to \"True\" to connect the first layer to the subsequent LSTM layer.\n"
      ],
      "metadata": {
        "id": "pYSWNB_HkIiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "data = pd.read_csv('/content/Inputs-Targets.csv')\n",
        "\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data['Year'] = data['Date'].dt.year\n",
        "data['Month'] = data['Date'].dt.month\n",
        "\n",
        "X = data.drop(['Date', '7P', '14P', '28P'], axis=1)\n",
        "y = data['7P']\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_encoded = encoder.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
        "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, return_sequences=True, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))  # Increased number of LSTM units\n",
        "model.add(Dropout(0.2))  # Dropout for regularization\n",
        "model.add(LSTM(50, return_sequences=False))  # Additional LSTM layer\n",
        "model.add(Dropout(0.2))  # Dropout for regularization\n",
        "model.add(Dense(5, activation='softmax'))  # Output layer for 5 categories\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)  # Adjusted learning rate\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train_reshaped, y_train, epochs=100, batch_size=64, validation_data=(X_test_reshaped, y_test), verbose=2)  # Increased epochs and adjusted batch size\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s47EQ7uQiO-b",
        "outputId": "b52d1415-7eeb-4411-e88f-8ee36b2ee9e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "682/682 - 10s - loss: 1.5284 - accuracy: 0.3376 - val_loss: 1.5120 - val_accuracy: 0.3520 - 10s/epoch - 14ms/step\n",
            "Epoch 2/100\n",
            "682/682 - 4s - loss: 1.4952 - accuracy: 0.3569 - val_loss: 1.4872 - val_accuracy: 0.3635 - 4s/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "682/682 - 5s - loss: 1.4686 - accuracy: 0.3742 - val_loss: 1.4617 - val_accuracy: 0.3791 - 5s/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "682/682 - 4s - loss: 1.4403 - accuracy: 0.3917 - val_loss: 1.4352 - val_accuracy: 0.3974 - 4s/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "682/682 - 4s - loss: 1.4117 - accuracy: 0.4082 - val_loss: 1.4086 - val_accuracy: 0.4119 - 4s/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "682/682 - 5s - loss: 1.3863 - accuracy: 0.4222 - val_loss: 1.3849 - val_accuracy: 0.4271 - 5s/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "682/682 - 4s - loss: 1.3622 - accuracy: 0.4347 - val_loss: 1.3613 - val_accuracy: 0.4412 - 4s/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "682/682 - 4s - loss: 1.3410 - accuracy: 0.4457 - val_loss: 1.3407 - val_accuracy: 0.4564 - 4s/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "682/682 - 6s - loss: 1.3168 - accuracy: 0.4583 - val_loss: 1.3149 - val_accuracy: 0.4673 - 6s/epoch - 8ms/step\n",
            "Epoch 10/100\n",
            "682/682 - 4s - loss: 1.3004 - accuracy: 0.4643 - val_loss: 1.2988 - val_accuracy: 0.4745 - 4s/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "682/682 - 4s - loss: 1.2786 - accuracy: 0.4770 - val_loss: 1.2790 - val_accuracy: 0.4841 - 4s/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "682/682 - 6s - loss: 1.2663 - accuracy: 0.4838 - val_loss: 1.2608 - val_accuracy: 0.4944 - 6s/epoch - 8ms/step\n",
            "Epoch 13/100\n",
            "682/682 - 4s - loss: 1.2457 - accuracy: 0.4919 - val_loss: 1.2474 - val_accuracy: 0.4996 - 4s/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "682/682 - 9s - loss: 1.2353 - accuracy: 0.4947 - val_loss: 1.2317 - val_accuracy: 0.5107 - 9s/epoch - 14ms/step\n",
            "Epoch 15/100\n",
            "682/682 - 6s - loss: 1.2165 - accuracy: 0.5057 - val_loss: 1.2155 - val_accuracy: 0.5143 - 6s/epoch - 9ms/step\n",
            "Epoch 16/100\n",
            "682/682 - 4s - loss: 1.2078 - accuracy: 0.5095 - val_loss: 1.2068 - val_accuracy: 0.5231 - 4s/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "682/682 - 5s - loss: 1.1941 - accuracy: 0.5165 - val_loss: 1.1911 - val_accuracy: 0.5284 - 5s/epoch - 7ms/step\n",
            "Epoch 18/100\n",
            "682/682 - 4s - loss: 1.1841 - accuracy: 0.5223 - val_loss: 1.1796 - val_accuracy: 0.5298 - 4s/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "682/682 - 4s - loss: 1.1713 - accuracy: 0.5303 - val_loss: 1.1685 - val_accuracy: 0.5387 - 4s/epoch - 7ms/step\n",
            "Epoch 20/100\n",
            "682/682 - 5s - loss: 1.1645 - accuracy: 0.5290 - val_loss: 1.1564 - val_accuracy: 0.5462 - 5s/epoch - 8ms/step\n",
            "Epoch 21/100\n",
            "682/682 - 6s - loss: 1.1499 - accuracy: 0.5377 - val_loss: 1.1443 - val_accuracy: 0.5499 - 6s/epoch - 9ms/step\n",
            "Epoch 22/100\n",
            "682/682 - 6s - loss: 1.1435 - accuracy: 0.5400 - val_loss: 1.1363 - val_accuracy: 0.5509 - 6s/epoch - 8ms/step\n",
            "Epoch 23/100\n",
            "682/682 - 4s - loss: 1.1311 - accuracy: 0.5468 - val_loss: 1.1255 - val_accuracy: 0.5593 - 4s/epoch - 6ms/step\n",
            "Epoch 24/100\n",
            "682/682 - 4s - loss: 1.1263 - accuracy: 0.5475 - val_loss: 1.1164 - val_accuracy: 0.5650 - 4s/epoch - 6ms/step\n",
            "Epoch 25/100\n",
            "682/682 - 6s - loss: 1.1197 - accuracy: 0.5511 - val_loss: 1.1085 - val_accuracy: 0.5673 - 6s/epoch - 8ms/step\n",
            "Epoch 26/100\n",
            "682/682 - 4s - loss: 1.1080 - accuracy: 0.5587 - val_loss: 1.0981 - val_accuracy: 0.5735 - 4s/epoch - 6ms/step\n",
            "Epoch 27/100\n",
            "682/682 - 4s - loss: 1.0986 - accuracy: 0.5627 - val_loss: 1.0916 - val_accuracy: 0.5794 - 4s/epoch - 6ms/step\n",
            "Epoch 28/100\n",
            "682/682 - 5s - loss: 1.0910 - accuracy: 0.5625 - val_loss: 1.0823 - val_accuracy: 0.5810 - 5s/epoch - 8ms/step\n",
            "Epoch 29/100\n",
            "682/682 - 4s - loss: 1.0898 - accuracy: 0.5657 - val_loss: 1.0750 - val_accuracy: 0.5852 - 4s/epoch - 6ms/step\n",
            "Epoch 30/100\n",
            "682/682 - 7s - loss: 1.0797 - accuracy: 0.5694 - val_loss: 1.0664 - val_accuracy: 0.5843 - 7s/epoch - 10ms/step\n",
            "Epoch 31/100\n",
            "682/682 - 6s - loss: 1.0717 - accuracy: 0.5744 - val_loss: 1.0588 - val_accuracy: 0.5935 - 6s/epoch - 10ms/step\n",
            "Epoch 32/100\n",
            "682/682 - 7s - loss: 1.0657 - accuracy: 0.5771 - val_loss: 1.0529 - val_accuracy: 0.5941 - 7s/epoch - 10ms/step\n",
            "Epoch 33/100\n",
            "682/682 - 7s - loss: 1.0619 - accuracy: 0.5789 - val_loss: 1.0457 - val_accuracy: 0.5984 - 7s/epoch - 10ms/step\n",
            "Epoch 34/100\n",
            "682/682 - 4s - loss: 1.0592 - accuracy: 0.5790 - val_loss: 1.0377 - val_accuracy: 0.6033 - 4s/epoch - 6ms/step\n",
            "Epoch 35/100\n",
            "682/682 - 6s - loss: 1.0516 - accuracy: 0.5844 - val_loss: 1.0337 - val_accuracy: 0.6068 - 6s/epoch - 8ms/step\n",
            "Epoch 36/100\n",
            "682/682 - 4s - loss: 1.0449 - accuracy: 0.5854 - val_loss: 1.0305 - val_accuracy: 0.6040 - 4s/epoch - 6ms/step\n",
            "Epoch 37/100\n",
            "682/682 - 4s - loss: 1.0441 - accuracy: 0.5853 - val_loss: 1.0228 - val_accuracy: 0.6117 - 4s/epoch - 6ms/step\n",
            "Epoch 38/100\n",
            "682/682 - 5s - loss: 1.0365 - accuracy: 0.5904 - val_loss: 1.0152 - val_accuracy: 0.6118 - 5s/epoch - 8ms/step\n",
            "Epoch 39/100\n",
            "682/682 - 4s - loss: 1.0319 - accuracy: 0.5937 - val_loss: 1.0130 - val_accuracy: 0.6128 - 4s/epoch - 6ms/step\n",
            "Epoch 40/100\n",
            "682/682 - 4s - loss: 1.0295 - accuracy: 0.5954 - val_loss: 1.0054 - val_accuracy: 0.6152 - 4s/epoch - 6ms/step\n",
            "Epoch 41/100\n",
            "682/682 - 5s - loss: 1.0244 - accuracy: 0.5953 - val_loss: 1.0037 - val_accuracy: 0.6166 - 5s/epoch - 8ms/step\n",
            "Epoch 42/100\n",
            "682/682 - 4s - loss: 1.0190 - accuracy: 0.5985 - val_loss: 0.9969 - val_accuracy: 0.6204 - 4s/epoch - 6ms/step\n",
            "Epoch 43/100\n",
            "682/682 - 5s - loss: 1.0131 - accuracy: 0.5975 - val_loss: 0.9937 - val_accuracy: 0.6196 - 5s/epoch - 7ms/step\n",
            "Epoch 44/100\n",
            "682/682 - 5s - loss: 1.0107 - accuracy: 0.6014 - val_loss: 0.9875 - val_accuracy: 0.6227 - 5s/epoch - 7ms/step\n",
            "Epoch 45/100\n",
            "682/682 - 4s - loss: 1.0086 - accuracy: 0.6024 - val_loss: 0.9839 - val_accuracy: 0.6249 - 4s/epoch - 6ms/step\n",
            "Epoch 46/100\n",
            "682/682 - 5s - loss: 1.0074 - accuracy: 0.6044 - val_loss: 0.9837 - val_accuracy: 0.6294 - 5s/epoch - 7ms/step\n",
            "Epoch 47/100\n",
            "682/682 - 5s - loss: 1.0018 - accuracy: 0.6044 - val_loss: 0.9787 - val_accuracy: 0.6296 - 5s/epoch - 7ms/step\n",
            "Epoch 48/100\n",
            "682/682 - 4s - loss: 1.0023 - accuracy: 0.6037 - val_loss: 0.9749 - val_accuracy: 0.6338 - 4s/epoch - 6ms/step\n",
            "Epoch 49/100\n",
            "682/682 - 6s - loss: 0.9936 - accuracy: 0.6084 - val_loss: 0.9709 - val_accuracy: 0.6342 - 6s/epoch - 8ms/step\n",
            "Epoch 50/100\n",
            "682/682 - 6s - loss: 0.9927 - accuracy: 0.6072 - val_loss: 0.9638 - val_accuracy: 0.6366 - 6s/epoch - 8ms/step\n",
            "Epoch 51/100\n",
            "682/682 - 4s - loss: 0.9849 - accuracy: 0.6149 - val_loss: 0.9609 - val_accuracy: 0.6381 - 4s/epoch - 6ms/step\n",
            "Epoch 52/100\n",
            "682/682 - 6s - loss: 0.9858 - accuracy: 0.6103 - val_loss: 0.9579 - val_accuracy: 0.6411 - 6s/epoch - 8ms/step\n",
            "Epoch 53/100\n",
            "682/682 - 4s - loss: 0.9829 - accuracy: 0.6153 - val_loss: 0.9566 - val_accuracy: 0.6417 - 4s/epoch - 6ms/step\n",
            "Epoch 54/100\n",
            "682/682 - 4s - loss: 0.9747 - accuracy: 0.6181 - val_loss: 0.9526 - val_accuracy: 0.6432 - 4s/epoch - 6ms/step\n",
            "Epoch 55/100\n",
            "682/682 - 5s - loss: 0.9724 - accuracy: 0.6193 - val_loss: 0.9449 - val_accuracy: 0.6478 - 5s/epoch - 8ms/step\n",
            "Epoch 56/100\n",
            "682/682 - 6s - loss: 0.9692 - accuracy: 0.6193 - val_loss: 0.9460 - val_accuracy: 0.6478 - 6s/epoch - 8ms/step\n",
            "Epoch 57/100\n",
            "682/682 - 6s - loss: 0.9703 - accuracy: 0.6166 - val_loss: 0.9446 - val_accuracy: 0.6458 - 6s/epoch - 9ms/step\n",
            "Epoch 58/100\n",
            "682/682 - 4s - loss: 0.9634 - accuracy: 0.6198 - val_loss: 0.9373 - val_accuracy: 0.6511 - 4s/epoch - 6ms/step\n",
            "Epoch 59/100\n",
            "682/682 - 4s - loss: 0.9667 - accuracy: 0.6170 - val_loss: 0.9364 - val_accuracy: 0.6501 - 4s/epoch - 6ms/step\n",
            "Epoch 60/100\n",
            "682/682 - 6s - loss: 0.9612 - accuracy: 0.6228 - val_loss: 0.9342 - val_accuracy: 0.6539 - 6s/epoch - 8ms/step\n",
            "Epoch 61/100\n",
            "682/682 - 6s - loss: 0.9567 - accuracy: 0.6276 - val_loss: 0.9312 - val_accuracy: 0.6547 - 6s/epoch - 9ms/step\n",
            "Epoch 62/100\n",
            "682/682 - 5s - loss: 0.9588 - accuracy: 0.6253 - val_loss: 0.9288 - val_accuracy: 0.6573 - 5s/epoch - 7ms/step\n",
            "Epoch 63/100\n",
            "682/682 - 5s - loss: 0.9544 - accuracy: 0.6259 - val_loss: 0.9283 - val_accuracy: 0.6561 - 5s/epoch - 7ms/step\n",
            "Epoch 64/100\n",
            "682/682 - 4s - loss: 0.9525 - accuracy: 0.6283 - val_loss: 0.9262 - val_accuracy: 0.6574 - 4s/epoch - 6ms/step\n",
            "Epoch 65/100\n",
            "682/682 - 5s - loss: 0.9525 - accuracy: 0.6274 - val_loss: 0.9224 - val_accuracy: 0.6614 - 5s/epoch - 8ms/step\n",
            "Epoch 66/100\n",
            "682/682 - 5s - loss: 0.9463 - accuracy: 0.6288 - val_loss: 0.9195 - val_accuracy: 0.6634 - 5s/epoch - 7ms/step\n",
            "Epoch 67/100\n",
            "682/682 - 6s - loss: 0.9429 - accuracy: 0.6325 - val_loss: 0.9154 - val_accuracy: 0.6629 - 6s/epoch - 8ms/step\n",
            "Epoch 68/100\n",
            "682/682 - 6s - loss: 0.9415 - accuracy: 0.6324 - val_loss: 0.9127 - val_accuracy: 0.6677 - 6s/epoch - 8ms/step\n",
            "Epoch 69/100\n",
            "682/682 - 4s - loss: 0.9402 - accuracy: 0.6314 - val_loss: 0.9099 - val_accuracy: 0.6642 - 4s/epoch - 6ms/step\n",
            "Epoch 70/100\n",
            "682/682 - 7s - loss: 0.9371 - accuracy: 0.6337 - val_loss: 0.9097 - val_accuracy: 0.6663 - 7s/epoch - 11ms/step\n",
            "Epoch 71/100\n",
            "682/682 - 4s - loss: 0.9322 - accuracy: 0.6357 - val_loss: 0.9080 - val_accuracy: 0.6705 - 4s/epoch - 6ms/step\n",
            "Epoch 72/100\n",
            "682/682 - 4s - loss: 0.9302 - accuracy: 0.6372 - val_loss: 0.9062 - val_accuracy: 0.6661 - 4s/epoch - 6ms/step\n",
            "Epoch 73/100\n",
            "682/682 - 6s - loss: 0.9380 - accuracy: 0.6338 - val_loss: 0.9034 - val_accuracy: 0.6703 - 6s/epoch - 9ms/step\n",
            "Epoch 74/100\n",
            "682/682 - 6s - loss: 0.9301 - accuracy: 0.6387 - val_loss: 0.9042 - val_accuracy: 0.6699 - 6s/epoch - 9ms/step\n",
            "Epoch 75/100\n",
            "682/682 - 4s - loss: 0.9261 - accuracy: 0.6398 - val_loss: 0.8999 - val_accuracy: 0.6712 - 4s/epoch - 6ms/step\n",
            "Epoch 76/100\n",
            "682/682 - 4s - loss: 0.9260 - accuracy: 0.6398 - val_loss: 0.8976 - val_accuracy: 0.6728 - 4s/epoch - 6ms/step\n",
            "Epoch 77/100\n",
            "682/682 - 5s - loss: 0.9233 - accuracy: 0.6399 - val_loss: 0.8929 - val_accuracy: 0.6716 - 5s/epoch - 8ms/step\n",
            "Epoch 78/100\n",
            "682/682 - 4s - loss: 0.9259 - accuracy: 0.6400 - val_loss: 0.8926 - val_accuracy: 0.6704 - 4s/epoch - 6ms/step\n",
            "Epoch 79/100\n",
            "682/682 - 4s - loss: 0.9256 - accuracy: 0.6406 - val_loss: 0.8914 - val_accuracy: 0.6726 - 4s/epoch - 6ms/step\n",
            "Epoch 80/100\n",
            "682/682 - 5s - loss: 0.9202 - accuracy: 0.6411 - val_loss: 0.8901 - val_accuracy: 0.6717 - 5s/epoch - 8ms/step\n",
            "Epoch 81/100\n",
            "682/682 - 4s - loss: 0.9196 - accuracy: 0.6421 - val_loss: 0.8880 - val_accuracy: 0.6751 - 4s/epoch - 6ms/step\n",
            "Epoch 82/100\n",
            "682/682 - 5s - loss: 0.9117 - accuracy: 0.6463 - val_loss: 0.8877 - val_accuracy: 0.6738 - 5s/epoch - 8ms/step\n",
            "Epoch 83/100\n",
            "682/682 - 5s - loss: 0.9165 - accuracy: 0.6440 - val_loss: 0.8871 - val_accuracy: 0.6738 - 5s/epoch - 7ms/step\n",
            "Epoch 84/100\n",
            "682/682 - 4s - loss: 0.9190 - accuracy: 0.6405 - val_loss: 0.8852 - val_accuracy: 0.6748 - 4s/epoch - 6ms/step\n",
            "Epoch 85/100\n",
            "682/682 - 6s - loss: 0.9175 - accuracy: 0.6419 - val_loss: 0.8834 - val_accuracy: 0.6788 - 6s/epoch - 8ms/step\n",
            "Epoch 86/100\n",
            "682/682 - 4s - loss: 0.9146 - accuracy: 0.6463 - val_loss: 0.8825 - val_accuracy: 0.6782 - 4s/epoch - 6ms/step\n",
            "Epoch 87/100\n",
            "682/682 - 4s - loss: 0.9052 - accuracy: 0.6481 - val_loss: 0.8807 - val_accuracy: 0.6802 - 4s/epoch - 6ms/step\n",
            "Epoch 88/100\n",
            "682/682 - 6s - loss: 0.9076 - accuracy: 0.6455 - val_loss: 0.8794 - val_accuracy: 0.6819 - 6s/epoch - 8ms/step\n",
            "Epoch 89/100\n",
            "682/682 - 4s - loss: 0.9060 - accuracy: 0.6461 - val_loss: 0.8768 - val_accuracy: 0.6778 - 4s/epoch - 6ms/step\n",
            "Epoch 90/100\n",
            "682/682 - 4s - loss: 0.9064 - accuracy: 0.6493 - val_loss: 0.8732 - val_accuracy: 0.6780 - 4s/epoch - 6ms/step\n",
            "Epoch 91/100\n",
            "682/682 - 6s - loss: 0.9048 - accuracy: 0.6480 - val_loss: 0.8709 - val_accuracy: 0.6783 - 6s/epoch - 8ms/step\n",
            "Epoch 92/100\n",
            "682/682 - 5s - loss: 0.9050 - accuracy: 0.6481 - val_loss: 0.8716 - val_accuracy: 0.6828 - 5s/epoch - 7ms/step\n",
            "Epoch 93/100\n",
            "682/682 - 4s - loss: 0.8997 - accuracy: 0.6506 - val_loss: 0.8694 - val_accuracy: 0.6826 - 4s/epoch - 6ms/step\n",
            "Epoch 94/100\n",
            "682/682 - 6s - loss: 0.8974 - accuracy: 0.6492 - val_loss: 0.8692 - val_accuracy: 0.6832 - 6s/epoch - 8ms/step\n",
            "Epoch 95/100\n",
            "682/682 - 4s - loss: 0.8998 - accuracy: 0.6527 - val_loss: 0.8704 - val_accuracy: 0.6804 - 4s/epoch - 6ms/step\n",
            "Epoch 96/100\n",
            "682/682 - 5s - loss: 0.9045 - accuracy: 0.6493 - val_loss: 0.8708 - val_accuracy: 0.6820 - 5s/epoch - 8ms/step\n",
            "Epoch 97/100\n",
            "682/682 - 4s - loss: 0.8951 - accuracy: 0.6559 - val_loss: 0.8642 - val_accuracy: 0.6834 - 4s/epoch - 6ms/step\n",
            "Epoch 98/100\n",
            "682/682 - 4s - loss: 0.8975 - accuracy: 0.6531 - val_loss: 0.8634 - val_accuracy: 0.6869 - 4s/epoch - 6ms/step\n",
            "Epoch 99/100\n",
            "682/682 - 5s - loss: 0.8992 - accuracy: 0.6508 - val_loss: 0.8638 - val_accuracy: 0.6873 - 5s/epoch - 8ms/step\n",
            "Epoch 100/100\n",
            "682/682 - 5s - loss: 0.8932 - accuracy: 0.6528 - val_loss: 0.8637 - val_accuracy: 0.6833 - 5s/epoch - 8ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d0ec99da350>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred = model.predict(X_test_reshaped)\n",
        "y_pred_classes = y_pred.argmax(axis=1)\n",
        "y_true = y_test.argmax(axis=1)\n",
        "\n",
        "# Confusion Matrix\n",
        "print(confusion_matrix(y_true, y_pred_classes))\n",
        "\n",
        "# Classification Report\n",
        "print(classification_report(y_true, y_pred_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SQH9TVOlO7q",
        "outputId": "16450d86-b530-4854-ee94-74f35776a8cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "341/341 [==============================] - 3s 5ms/step\n",
            "[[2354  329  109   93  106]\n",
            " [ 388 1211  205  161  148]\n",
            " [ 201  187  754  213  180]\n",
            " [ 130   97  154 1018  405]\n",
            " [  46   41   47  215 2117]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.79      0.77      2991\n",
            "           1       0.65      0.57      0.61      2113\n",
            "           2       0.59      0.49      0.54      1535\n",
            "           3       0.60      0.56      0.58      1804\n",
            "           4       0.72      0.86      0.78      2466\n",
            "\n",
            "    accuracy                           0.68     10909\n",
            "   macro avg       0.66      0.65      0.66     10909\n",
            "weighted avg       0.68      0.68      0.68     10909\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analysis\n",
        "**Training and Validation Loss and Accuracy:**\n",
        "- There is a steady decrease in loss and increase in accuracy over the 100 epochs, which is a good sign\n",
        "- The validation accuracy has improved to 68.3%, which is a good increase from the previous model.\n",
        "- There are no signs of overfitting since the validation loss and accuracy are in line with the training loss and accuracy\n",
        "\n",
        "**Confusion Matrix:**\n",
        "- The diagonal elements represent the true positive predictions for each class (0-4). They are higher than the previous model, which is a good sign.\n",
        "- The misclassification showed a decrease as well, which means that the model is making fewer mistakes\n",
        "\n",
        "**Classification Report Analysis:**\n",
        "\n",
        "- Precision: precision is high for classes 0 and 4, which means there are fewer false positives. As for classes 1,2, and 3, they have moderate precision.\n",
        "- Recall: Class 4 has the highest recall, which is 0.86. This means that the model is very effective in predicting this category. As for classes 0,1,2, and 3, they have moderate precision.\n",
        "- F1 Score: This score combines precision and recall. The scorer is high for classes 0 and 4, which means that there is a good balance between precision and recall for these categories. The rest of the classes have moderate F1 scores.\n",
        "\n"
      ],
      "metadata": {
        "id": "rd_C9RiFlbFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Steps:**\n",
        "\n",
        "There are multiple steps that could be taken at this point to improve the performance of the model. These include more modifications to the model like increasing the number of epochs, adding additional LSTM layers, and dropout for regularization. <br>\n",
        "\n",
        "However, for the next step, I am going to experiment with training the model using cross-validation, to ensure that the model is robust."
      ],
      "metadata": {
        "id": "mk1wsQ0-pXMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "data = pd.read_csv('/content/Inputs-Targets.csv')\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data['Year'] = data['Date'].dt.year\n",
        "data['Month'] = data['Date'].dt.month\n",
        "X = data.drop(['Date', '7P', '14P', '28P'], axis=1)\n",
        "y = data['7P']\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_encoded = encoder.fit_transform(y.values.reshape(-1, 1))\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "def create_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(100, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(50, return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "fold_no = 1\n",
        "\n",
        "for train_index, test_index in tscv.split(X_scaled):\n",
        "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
        "    y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
        "\n",
        "    X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "    X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "    model = create_model((X_train_reshaped.shape[1], X_train_reshaped.shape[2]))\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "    model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, verbose=2)\n",
        "\n",
        "    scores = model.evaluate(X_test_reshaped, y_test, verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "\n",
        "    fold_no += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55yrPqXJmCsG",
        "outputId": "d8163b4c-6eb9-4af9-aaaf-4e9012a93848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for fold 1 ...\n",
            "Epoch 1/50\n",
            "285/285 - 6s - loss: 1.5035 - accuracy: 0.3675 - 6s/epoch - 20ms/step\n",
            "Epoch 2/50\n",
            "285/285 - 2s - loss: 1.4454 - accuracy: 0.4061 - 2s/epoch - 5ms/step\n",
            "Epoch 3/50\n",
            "285/285 - 2s - loss: 1.3961 - accuracy: 0.4296 - 2s/epoch - 6ms/step\n",
            "Epoch 4/50\n",
            "285/285 - 2s - loss: 1.3465 - accuracy: 0.4590 - 2s/epoch - 6ms/step\n",
            "Epoch 5/50\n",
            "285/285 - 1s - loss: 1.2953 - accuracy: 0.4823 - 1s/epoch - 5ms/step\n",
            "Epoch 6/50\n",
            "285/285 - 1s - loss: 1.2413 - accuracy: 0.5064 - 1s/epoch - 5ms/step\n",
            "Epoch 7/50\n",
            "285/285 - 1s - loss: 1.1899 - accuracy: 0.5255 - 1s/epoch - 5ms/step\n",
            "Epoch 8/50\n",
            "285/285 - 1s - loss: 1.1502 - accuracy: 0.5448 - 1s/epoch - 5ms/step\n",
            "Epoch 9/50\n",
            "285/285 - 1s - loss: 1.1074 - accuracy: 0.5634 - 1s/epoch - 5ms/step\n",
            "Epoch 10/50\n",
            "285/285 - 1s - loss: 1.0690 - accuracy: 0.5796 - 1s/epoch - 5ms/step\n",
            "Epoch 11/50\n",
            "285/285 - 1s - loss: 1.0340 - accuracy: 0.6014 - 1s/epoch - 5ms/step\n",
            "Epoch 12/50\n",
            "285/285 - 2s - loss: 1.0006 - accuracy: 0.6050 - 2s/epoch - 7ms/step\n",
            "Epoch 13/50\n",
            "285/285 - 2s - loss: 0.9824 - accuracy: 0.6145 - 2s/epoch - 6ms/step\n",
            "Epoch 14/50\n",
            "285/285 - 1s - loss: 0.9532 - accuracy: 0.6313 - 1s/epoch - 5ms/step\n",
            "Epoch 15/50\n",
            "285/285 - 1s - loss: 0.9332 - accuracy: 0.6344 - 1s/epoch - 4ms/step\n",
            "Epoch 16/50\n",
            "285/285 - 1s - loss: 0.8952 - accuracy: 0.6586 - 1s/epoch - 4ms/step\n",
            "Epoch 17/50\n",
            "285/285 - 1s - loss: 0.8774 - accuracy: 0.6670 - 1s/epoch - 5ms/step\n",
            "Epoch 18/50\n",
            "285/285 - 1s - loss: 0.8583 - accuracy: 0.6701 - 1s/epoch - 5ms/step\n",
            "Epoch 19/50\n",
            "285/285 - 1s - loss: 0.8320 - accuracy: 0.6774 - 1s/epoch - 5ms/step\n",
            "Epoch 20/50\n",
            "285/285 - 3s - loss: 0.8260 - accuracy: 0.6847 - 3s/epoch - 10ms/step\n",
            "Epoch 21/50\n",
            "285/285 - 3s - loss: 0.8180 - accuracy: 0.6843 - 3s/epoch - 10ms/step\n",
            "Epoch 22/50\n",
            "285/285 - 2s - loss: 0.8085 - accuracy: 0.6952 - 2s/epoch - 7ms/step\n",
            "Epoch 23/50\n",
            "285/285 - 2s - loss: 0.7892 - accuracy: 0.7009 - 2s/epoch - 5ms/step\n",
            "Epoch 24/50\n",
            "285/285 - 1s - loss: 0.7773 - accuracy: 0.7065 - 1s/epoch - 5ms/step\n",
            "Epoch 25/50\n",
            "285/285 - 1s - loss: 0.7568 - accuracy: 0.7155 - 1s/epoch - 5ms/step\n",
            "Epoch 26/50\n",
            "285/285 - 1s - loss: 0.7484 - accuracy: 0.7155 - 1s/epoch - 5ms/step\n",
            "Epoch 27/50\n",
            "285/285 - 1s - loss: 0.7403 - accuracy: 0.7179 - 1s/epoch - 5ms/step\n",
            "Epoch 28/50\n",
            "285/285 - 2s - loss: 0.7294 - accuracy: 0.7243 - 2s/epoch - 5ms/step\n",
            "Epoch 29/50\n",
            "285/285 - 2s - loss: 0.7243 - accuracy: 0.7230 - 2s/epoch - 7ms/step\n",
            "Epoch 30/50\n",
            "285/285 - 2s - loss: 0.7111 - accuracy: 0.7358 - 2s/epoch - 6ms/step\n",
            "Epoch 31/50\n",
            "285/285 - 1s - loss: 0.7069 - accuracy: 0.7288 - 1s/epoch - 5ms/step\n",
            "Epoch 32/50\n",
            "285/285 - 1s - loss: 0.7035 - accuracy: 0.7361 - 1s/epoch - 5ms/step\n",
            "Epoch 33/50\n",
            "285/285 - 1s - loss: 0.6853 - accuracy: 0.7437 - 1s/epoch - 5ms/step\n",
            "Epoch 34/50\n",
            "285/285 - 1s - loss: 0.6858 - accuracy: 0.7416 - 1s/epoch - 5ms/step\n",
            "Epoch 35/50\n",
            "285/285 - 1s - loss: 0.6710 - accuracy: 0.7514 - 1s/epoch - 5ms/step\n",
            "Epoch 36/50\n",
            "285/285 - 1s - loss: 0.6704 - accuracy: 0.7453 - 1s/epoch - 5ms/step\n",
            "Epoch 37/50\n",
            "285/285 - 1s - loss: 0.6488 - accuracy: 0.7562 - 1s/epoch - 5ms/step\n",
            "Epoch 38/50\n",
            "285/285 - 2s - loss: 0.6479 - accuracy: 0.7576 - 2s/epoch - 6ms/step\n",
            "Epoch 39/50\n",
            "285/285 - 2s - loss: 0.6416 - accuracy: 0.7624 - 2s/epoch - 6ms/step\n",
            "Epoch 40/50\n",
            "285/285 - 1s - loss: 0.6320 - accuracy: 0.7596 - 1s/epoch - 5ms/step\n",
            "Epoch 41/50\n",
            "285/285 - 1s - loss: 0.6216 - accuracy: 0.7661 - 1s/epoch - 4ms/step\n",
            "Epoch 42/50\n",
            "285/285 - 1s - loss: 0.6233 - accuracy: 0.7680 - 1s/epoch - 4ms/step\n",
            "Epoch 43/50\n",
            "285/285 - 1s - loss: 0.6299 - accuracy: 0.7619 - 1s/epoch - 5ms/step\n",
            "Epoch 44/50\n",
            "285/285 - 1s - loss: 0.6048 - accuracy: 0.7788 - 1s/epoch - 5ms/step\n",
            "Epoch 45/50\n",
            "285/285 - 1s - loss: 0.6080 - accuracy: 0.7774 - 1s/epoch - 5ms/step\n",
            "Epoch 46/50\n",
            "285/285 - 1s - loss: 0.6044 - accuracy: 0.7720 - 1s/epoch - 5ms/step\n",
            "Epoch 47/50\n",
            "285/285 - 2s - loss: 0.5945 - accuracy: 0.7761 - 2s/epoch - 6ms/step\n",
            "Epoch 48/50\n",
            "285/285 - 2s - loss: 0.5944 - accuracy: 0.7769 - 2s/epoch - 7ms/step\n",
            "Epoch 49/50\n",
            "285/285 - 1s - loss: 0.5899 - accuracy: 0.7796 - 1s/epoch - 5ms/step\n",
            "Epoch 50/50\n",
            "285/285 - 1s - loss: 0.5847 - accuracy: 0.7791 - 1s/epoch - 4ms/step\n",
            "Score for fold 1: loss of 2.571263313293457; accuracy of 22.673267126083374%\n",
            "Training for fold 2 ...\n",
            "Epoch 1/50\n",
            "569/569 - 6s - loss: 1.5196 - accuracy: 0.3456 - 6s/epoch - 11ms/step\n",
            "Epoch 2/50\n",
            "569/569 - 5s - loss: 1.4666 - accuracy: 0.3799 - 5s/epoch - 8ms/step\n",
            "Epoch 3/50\n",
            "569/569 - 3s - loss: 1.4160 - accuracy: 0.4081 - 3s/epoch - 6ms/step\n",
            "Epoch 4/50\n",
            "569/569 - 3s - loss: 1.3661 - accuracy: 0.4362 - 3s/epoch - 5ms/step\n",
            "Epoch 5/50\n",
            "569/569 - 3s - loss: 1.3105 - accuracy: 0.4624 - 3s/epoch - 5ms/step\n",
            "Epoch 6/50\n",
            "569/569 - 3s - loss: 1.2675 - accuracy: 0.4858 - 3s/epoch - 6ms/step\n",
            "Epoch 7/50\n",
            "569/569 - 3s - loss: 1.2249 - accuracy: 0.5087 - 3s/epoch - 4ms/step\n",
            "Epoch 8/50\n",
            "569/569 - 3s - loss: 1.1844 - accuracy: 0.5251 - 3s/epoch - 4ms/step\n",
            "Epoch 9/50\n",
            "569/569 - 3s - loss: 1.1483 - accuracy: 0.5424 - 3s/epoch - 4ms/step\n",
            "Epoch 10/50\n",
            "569/569 - 3s - loss: 1.1137 - accuracy: 0.5582 - 3s/epoch - 5ms/step\n",
            "Epoch 11/50\n",
            "569/569 - 3s - loss: 1.0863 - accuracy: 0.5712 - 3s/epoch - 6ms/step\n",
            "Epoch 12/50\n",
            "569/569 - 3s - loss: 1.0630 - accuracy: 0.5798 - 3s/epoch - 4ms/step\n",
            "Epoch 13/50\n",
            "569/569 - 3s - loss: 1.0381 - accuracy: 0.5933 - 3s/epoch - 4ms/step\n",
            "Epoch 14/50\n",
            "569/569 - 3s - loss: 1.0236 - accuracy: 0.6017 - 3s/epoch - 6ms/step\n",
            "Epoch 15/50\n",
            "569/569 - 5s - loss: 0.9997 - accuracy: 0.6097 - 5s/epoch - 9ms/step\n",
            "Epoch 16/50\n",
            "569/569 - 4s - loss: 0.9783 - accuracy: 0.6226 - 4s/epoch - 7ms/step\n",
            "Epoch 17/50\n",
            "569/569 - 3s - loss: 0.9659 - accuracy: 0.6263 - 3s/epoch - 4ms/step\n",
            "Epoch 18/50\n",
            "569/569 - 3s - loss: 0.9494 - accuracy: 0.6340 - 3s/epoch - 5ms/step\n",
            "Epoch 19/50\n",
            "569/569 - 4s - loss: 0.9346 - accuracy: 0.6415 - 4s/epoch - 6ms/step\n",
            "Epoch 20/50\n",
            "569/569 - 3s - loss: 0.9143 - accuracy: 0.6456 - 3s/epoch - 5ms/step\n",
            "Epoch 21/50\n",
            "569/569 - 3s - loss: 0.8950 - accuracy: 0.6605 - 3s/epoch - 5ms/step\n",
            "Epoch 22/50\n",
            "569/569 - 2s - loss: 0.8960 - accuracy: 0.6541 - 2s/epoch - 4ms/step\n",
            "Epoch 23/50\n",
            "569/569 - 3s - loss: 0.8718 - accuracy: 0.6671 - 3s/epoch - 4ms/step\n",
            "Epoch 24/50\n",
            "569/569 - 3s - loss: 0.8645 - accuracy: 0.6689 - 3s/epoch - 6ms/step\n",
            "Epoch 25/50\n",
            "569/569 - 3s - loss: 0.8558 - accuracy: 0.6754 - 3s/epoch - 5ms/step\n",
            "Epoch 26/50\n",
            "569/569 - 3s - loss: 0.8445 - accuracy: 0.6797 - 3s/epoch - 4ms/step\n",
            "Epoch 27/50\n",
            "569/569 - 3s - loss: 0.8346 - accuracy: 0.6828 - 3s/epoch - 5ms/step\n",
            "Epoch 28/50\n",
            "569/569 - 3s - loss: 0.8249 - accuracy: 0.6832 - 3s/epoch - 5ms/step\n",
            "Epoch 29/50\n",
            "569/569 - 3s - loss: 0.8117 - accuracy: 0.6897 - 3s/epoch - 6ms/step\n",
            "Epoch 30/50\n",
            "569/569 - 3s - loss: 0.7983 - accuracy: 0.6956 - 3s/epoch - 5ms/step\n",
            "Epoch 31/50\n",
            "569/569 - 3s - loss: 0.8022 - accuracy: 0.6944 - 3s/epoch - 4ms/step\n",
            "Epoch 32/50\n",
            "569/569 - 3s - loss: 0.7870 - accuracy: 0.7024 - 3s/epoch - 5ms/step\n",
            "Epoch 33/50\n",
            "569/569 - 3s - loss: 0.7742 - accuracy: 0.7046 - 3s/epoch - 5ms/step\n",
            "Epoch 34/50\n",
            "569/569 - 3s - loss: 0.7805 - accuracy: 0.7047 - 3s/epoch - 6ms/step\n",
            "Epoch 35/50\n",
            "569/569 - 2s - loss: 0.7773 - accuracy: 0.7075 - 2s/epoch - 4ms/step\n",
            "Epoch 36/50\n",
            "569/569 - 3s - loss: 0.7642 - accuracy: 0.7125 - 3s/epoch - 5ms/step\n",
            "Epoch 37/50\n",
            "569/569 - 3s - loss: 0.7596 - accuracy: 0.7129 - 3s/epoch - 5ms/step\n",
            "Epoch 38/50\n",
            "569/569 - 3s - loss: 0.7575 - accuracy: 0.7112 - 3s/epoch - 6ms/step\n",
            "Epoch 39/50\n",
            "569/569 - 3s - loss: 0.7476 - accuracy: 0.7187 - 3s/epoch - 5ms/step\n",
            "Epoch 40/50\n",
            "569/569 - 3s - loss: 0.7416 - accuracy: 0.7209 - 3s/epoch - 5ms/step\n",
            "Epoch 41/50\n",
            "569/569 - 4s - loss: 0.7379 - accuracy: 0.7206 - 4s/epoch - 7ms/step\n",
            "Epoch 42/50\n",
            "569/569 - 4s - loss: 0.7339 - accuracy: 0.7215 - 4s/epoch - 7ms/step\n",
            "Epoch 43/50\n",
            "569/569 - 3s - loss: 0.7224 - accuracy: 0.7255 - 3s/epoch - 6ms/step\n",
            "Epoch 44/50\n",
            "569/569 - 3s - loss: 0.7203 - accuracy: 0.7278 - 3s/epoch - 5ms/step\n",
            "Epoch 45/50\n",
            "569/569 - 3s - loss: 0.7191 - accuracy: 0.7254 - 3s/epoch - 5ms/step\n",
            "Epoch 46/50\n",
            "569/569 - 3s - loss: 0.7127 - accuracy: 0.7303 - 3s/epoch - 5ms/step\n",
            "Epoch 47/50\n",
            "569/569 - 4s - loss: 0.7130 - accuracy: 0.7331 - 4s/epoch - 6ms/step\n",
            "Epoch 48/50\n",
            "569/569 - 3s - loss: 0.6995 - accuracy: 0.7378 - 3s/epoch - 5ms/step\n",
            "Epoch 49/50\n",
            "569/569 - 3s - loss: 0.7022 - accuracy: 0.7355 - 3s/epoch - 5ms/step\n",
            "Epoch 50/50\n",
            "569/569 - 3s - loss: 0.6987 - accuracy: 0.7349 - 3s/epoch - 5ms/step\n",
            "Score for fold 2: loss of 2.4232378005981445; accuracy of 23.773376643657684%\n",
            "Training for fold 3 ...\n",
            "Epoch 1/50\n",
            "853/853 - 8s - loss: 1.5245 - accuracy: 0.3415 - 8s/epoch - 10ms/step\n",
            "Epoch 2/50\n",
            "853/853 - 5s - loss: 1.4769 - accuracy: 0.3683 - 5s/epoch - 5ms/step\n",
            "Epoch 3/50\n",
            "853/853 - 4s - loss: 1.4333 - accuracy: 0.3963 - 4s/epoch - 5ms/step\n",
            "Epoch 4/50\n",
            "853/853 - 5s - loss: 1.3839 - accuracy: 0.4267 - 5s/epoch - 5ms/step\n",
            "Epoch 5/50\n",
            "853/853 - 4s - loss: 1.3374 - accuracy: 0.4502 - 4s/epoch - 5ms/step\n",
            "Epoch 6/50\n",
            "853/853 - 4s - loss: 1.2938 - accuracy: 0.4707 - 4s/epoch - 4ms/step\n",
            "Epoch 7/50\n",
            "853/853 - 8s - loss: 1.2581 - accuracy: 0.4876 - 8s/epoch - 9ms/step\n",
            "Epoch 8/50\n",
            "853/853 - 5s - loss: 1.2180 - accuracy: 0.5090 - 5s/epoch - 6ms/step\n",
            "Epoch 9/50\n",
            "853/853 - 4s - loss: 1.1911 - accuracy: 0.5176 - 4s/epoch - 5ms/step\n",
            "Epoch 10/50\n",
            "853/853 - 5s - loss: 1.1591 - accuracy: 0.5345 - 5s/epoch - 6ms/step\n",
            "Epoch 11/50\n",
            "853/853 - 4s - loss: 1.1283 - accuracy: 0.5479 - 4s/epoch - 5ms/step\n",
            "Epoch 12/50\n",
            "853/853 - 4s - loss: 1.1128 - accuracy: 0.5576 - 4s/epoch - 5ms/step\n",
            "Epoch 13/50\n",
            "853/853 - 5s - loss: 1.0877 - accuracy: 0.5668 - 5s/epoch - 6ms/step\n",
            "Epoch 14/50\n",
            "853/853 - 5s - loss: 1.0674 - accuracy: 0.5784 - 5s/epoch - 5ms/step\n",
            "Epoch 15/50\n",
            "853/853 - 4s - loss: 1.0576 - accuracy: 0.5822 - 4s/epoch - 5ms/step\n",
            "Epoch 16/50\n",
            "853/853 - 6s - loss: 1.0362 - accuracy: 0.5913 - 6s/epoch - 7ms/step\n",
            "Epoch 17/50\n",
            "853/853 - 5s - loss: 1.0242 - accuracy: 0.5963 - 5s/epoch - 6ms/step\n",
            "Epoch 18/50\n",
            "853/853 - 7s - loss: 1.0027 - accuracy: 0.6086 - 7s/epoch - 8ms/step\n",
            "Epoch 19/50\n",
            "853/853 - 6s - loss: 0.9899 - accuracy: 0.6100 - 6s/epoch - 7ms/step\n",
            "Epoch 20/50\n",
            "853/853 - 6s - loss: 0.9767 - accuracy: 0.6177 - 6s/epoch - 7ms/step\n",
            "Epoch 21/50\n",
            "853/853 - 6s - loss: 0.9694 - accuracy: 0.6230 - 6s/epoch - 8ms/step\n",
            "Epoch 22/50\n",
            "853/853 - 4s - loss: 0.9557 - accuracy: 0.6316 - 4s/epoch - 5ms/step\n",
            "Epoch 23/50\n",
            "853/853 - 5s - loss: 0.9465 - accuracy: 0.6318 - 5s/epoch - 6ms/step\n",
            "Epoch 24/50\n",
            "853/853 - 4s - loss: 0.9354 - accuracy: 0.6397 - 4s/epoch - 5ms/step\n",
            "Epoch 25/50\n",
            "853/853 - 5s - loss: 0.9286 - accuracy: 0.6412 - 5s/epoch - 6ms/step\n",
            "Epoch 26/50\n",
            "853/853 - 7s - loss: 0.9204 - accuracy: 0.6440 - 7s/epoch - 9ms/step\n",
            "Epoch 27/50\n",
            "853/853 - 4s - loss: 0.9001 - accuracy: 0.6523 - 4s/epoch - 5ms/step\n",
            "Epoch 28/50\n",
            "853/853 - 5s - loss: 0.9019 - accuracy: 0.6516 - 5s/epoch - 5ms/step\n",
            "Epoch 29/50\n",
            "853/853 - 5s - loss: 0.8893 - accuracy: 0.6580 - 5s/epoch - 5ms/step\n",
            "Epoch 30/50\n",
            "853/853 - 4s - loss: 0.8931 - accuracy: 0.6559 - 4s/epoch - 5ms/step\n",
            "Epoch 31/50\n",
            "853/853 - 5s - loss: 0.8734 - accuracy: 0.6638 - 5s/epoch - 5ms/step\n",
            "Epoch 32/50\n",
            "853/853 - 5s - loss: 0.8678 - accuracy: 0.6648 - 5s/epoch - 5ms/step\n",
            "Epoch 33/50\n",
            "853/853 - 4s - loss: 0.8671 - accuracy: 0.6676 - 4s/epoch - 5ms/step\n",
            "Epoch 34/50\n",
            "853/853 - 5s - loss: 0.8565 - accuracy: 0.6703 - 5s/epoch - 6ms/step\n",
            "Epoch 35/50\n",
            "853/853 - 5s - loss: 0.8535 - accuracy: 0.6763 - 5s/epoch - 5ms/step\n",
            "Epoch 36/50\n",
            "853/853 - 4s - loss: 0.8469 - accuracy: 0.6775 - 4s/epoch - 5ms/step\n",
            "Epoch 37/50\n",
            "853/853 - 5s - loss: 0.8399 - accuracy: 0.6815 - 5s/epoch - 6ms/step\n",
            "Epoch 38/50\n",
            "853/853 - 5s - loss: 0.8351 - accuracy: 0.6818 - 5s/epoch - 5ms/step\n",
            "Epoch 39/50\n",
            "853/853 - 4s - loss: 0.8295 - accuracy: 0.6855 - 4s/epoch - 5ms/step\n",
            "Epoch 40/50\n",
            "853/853 - 5s - loss: 0.8192 - accuracy: 0.6866 - 5s/epoch - 6ms/step\n",
            "Epoch 41/50\n",
            "853/853 - 4s - loss: 0.8180 - accuracy: 0.6862 - 4s/epoch - 5ms/step\n",
            "Epoch 42/50\n",
            "853/853 - 4s - loss: 0.8129 - accuracy: 0.6888 - 4s/epoch - 5ms/step\n",
            "Epoch 43/50\n",
            "853/853 - 5s - loss: 0.8114 - accuracy: 0.6878 - 5s/epoch - 6ms/step\n",
            "Epoch 44/50\n",
            "853/853 - 4s - loss: 0.8017 - accuracy: 0.6929 - 4s/epoch - 5ms/step\n",
            "Epoch 45/50\n",
            "853/853 - 4s - loss: 0.8093 - accuracy: 0.6885 - 4s/epoch - 5ms/step\n",
            "Epoch 46/50\n",
            "853/853 - 5s - loss: 0.7987 - accuracy: 0.6943 - 5s/epoch - 6ms/step\n",
            "Epoch 47/50\n",
            "853/853 - 4s - loss: 0.7960 - accuracy: 0.6955 - 4s/epoch - 5ms/step\n",
            "Epoch 48/50\n",
            "853/853 - 4s - loss: 0.7918 - accuracy: 0.7004 - 4s/epoch - 5ms/step\n",
            "Epoch 49/50\n",
            "853/853 - 5s - loss: 0.7793 - accuracy: 0.7033 - 5s/epoch - 6ms/step\n",
            "Epoch 50/50\n",
            "853/853 - 4s - loss: 0.7788 - accuracy: 0.7054 - 4s/epoch - 5ms/step\n",
            "Score for fold 3: loss of 2.5602610111236572; accuracy of 24.53245371580124%\n",
            "Training for fold 4 ...\n",
            "Epoch 1/50\n",
            "1137/1137 - 11s - loss: 1.5214 - accuracy: 0.3373 - 11s/epoch - 9ms/step\n",
            "Epoch 2/50\n",
            "1137/1137 - 6s - loss: 1.4766 - accuracy: 0.3680 - 6s/epoch - 5ms/step\n",
            "Epoch 3/50\n",
            "1137/1137 - 6s - loss: 1.4296 - accuracy: 0.3946 - 6s/epoch - 5ms/step\n",
            "Epoch 4/50\n",
            "1137/1137 - 6s - loss: 1.3852 - accuracy: 0.4237 - 6s/epoch - 5ms/step\n",
            "Epoch 5/50\n",
            "1137/1137 - 7s - loss: 1.3425 - accuracy: 0.4430 - 7s/epoch - 6ms/step\n",
            "Epoch 6/50\n",
            "1137/1137 - 6s - loss: 1.3048 - accuracy: 0.4635 - 6s/epoch - 5ms/step\n",
            "Epoch 7/50\n",
            "1137/1137 - 7s - loss: 1.2735 - accuracy: 0.4778 - 7s/epoch - 6ms/step\n",
            "Epoch 8/50\n",
            "1137/1137 - 6s - loss: 1.2431 - accuracy: 0.4919 - 6s/epoch - 5ms/step\n",
            "Epoch 9/50\n",
            "1137/1137 - 7s - loss: 1.2141 - accuracy: 0.5065 - 7s/epoch - 6ms/step\n",
            "Epoch 10/50\n",
            "1137/1137 - 6s - loss: 1.1870 - accuracy: 0.5216 - 6s/epoch - 5ms/step\n",
            "Epoch 11/50\n",
            "1137/1137 - 7s - loss: 1.1643 - accuracy: 0.5307 - 7s/epoch - 6ms/step\n",
            "Epoch 12/50\n",
            "1137/1137 - 6s - loss: 1.1437 - accuracy: 0.5387 - 6s/epoch - 5ms/step\n",
            "Epoch 13/50\n",
            "1137/1137 - 6s - loss: 1.1209 - accuracy: 0.5514 - 6s/epoch - 6ms/step\n",
            "Epoch 14/50\n",
            "1137/1137 - 6s - loss: 1.1028 - accuracy: 0.5591 - 6s/epoch - 5ms/step\n",
            "Epoch 15/50\n",
            "1137/1137 - 6s - loss: 1.0867 - accuracy: 0.5665 - 6s/epoch - 5ms/step\n",
            "Epoch 16/50\n",
            "1137/1137 - 6s - loss: 1.0769 - accuracy: 0.5710 - 6s/epoch - 6ms/step\n",
            "Epoch 17/50\n",
            "1137/1137 - 6s - loss: 1.0622 - accuracy: 0.5766 - 6s/epoch - 5ms/step\n",
            "Epoch 18/50\n",
            "1137/1137 - 7s - loss: 1.0459 - accuracy: 0.5849 - 7s/epoch - 6ms/step\n",
            "Epoch 19/50\n",
            "1137/1137 - 5s - loss: 1.0358 - accuracy: 0.5931 - 5s/epoch - 5ms/step\n",
            "Epoch 20/50\n",
            "1137/1137 - 6s - loss: 1.0195 - accuracy: 0.5967 - 6s/epoch - 6ms/step\n",
            "Epoch 21/50\n",
            "1137/1137 - 5s - loss: 1.0154 - accuracy: 0.5991 - 5s/epoch - 5ms/step\n",
            "Epoch 22/50\n",
            "1137/1137 - 6s - loss: 1.0097 - accuracy: 0.6014 - 6s/epoch - 6ms/step\n",
            "Epoch 23/50\n",
            "1137/1137 - 6s - loss: 0.9964 - accuracy: 0.6077 - 6s/epoch - 5ms/step\n",
            "Epoch 24/50\n",
            "1137/1137 - 6s - loss: 0.9850 - accuracy: 0.6102 - 6s/epoch - 5ms/step\n",
            "Epoch 25/50\n",
            "1137/1137 - 6s - loss: 0.9755 - accuracy: 0.6178 - 6s/epoch - 5ms/step\n",
            "Epoch 26/50\n",
            "1137/1137 - 5s - loss: 0.9726 - accuracy: 0.6179 - 5s/epoch - 5ms/step\n",
            "Epoch 27/50\n",
            "1137/1137 - 6s - loss: 0.9639 - accuracy: 0.6233 - 6s/epoch - 6ms/step\n",
            "Epoch 28/50\n",
            "1137/1137 - 5s - loss: 0.9524 - accuracy: 0.6270 - 5s/epoch - 5ms/step\n",
            "Epoch 29/50\n",
            "1137/1137 - 6s - loss: 0.9455 - accuracy: 0.6321 - 6s/epoch - 5ms/step\n",
            "Epoch 30/50\n",
            "1137/1137 - 5s - loss: 0.9422 - accuracy: 0.6343 - 5s/epoch - 5ms/step\n",
            "Epoch 31/50\n",
            "1137/1137 - 5s - loss: 0.9313 - accuracy: 0.6358 - 5s/epoch - 5ms/step\n",
            "Epoch 32/50\n",
            "1137/1137 - 6s - loss: 0.9262 - accuracy: 0.6408 - 6s/epoch - 5ms/step\n",
            "Epoch 33/50\n",
            "1137/1137 - 5s - loss: 0.9177 - accuracy: 0.6419 - 5s/epoch - 5ms/step\n",
            "Epoch 34/50\n",
            "1137/1137 - 7s - loss: 0.9114 - accuracy: 0.6457 - 7s/epoch - 6ms/step\n",
            "Epoch 35/50\n",
            "1137/1137 - 6s - loss: 0.9110 - accuracy: 0.6450 - 6s/epoch - 5ms/step\n",
            "Epoch 36/50\n",
            "1137/1137 - 7s - loss: 0.9054 - accuracy: 0.6471 - 7s/epoch - 6ms/step\n",
            "Epoch 37/50\n",
            "1137/1137 - 6s - loss: 0.9019 - accuracy: 0.6506 - 6s/epoch - 5ms/step\n",
            "Epoch 38/50\n",
            "1137/1137 - 6s - loss: 0.8891 - accuracy: 0.6546 - 6s/epoch - 6ms/step\n",
            "Epoch 39/50\n",
            "1137/1137 - 6s - loss: 0.8922 - accuracy: 0.6546 - 6s/epoch - 5ms/step\n",
            "Epoch 40/50\n",
            "1137/1137 - 6s - loss: 0.8851 - accuracy: 0.6550 - 6s/epoch - 5ms/step\n",
            "Epoch 41/50\n",
            "1137/1137 - 6s - loss: 0.8820 - accuracy: 0.6596 - 6s/epoch - 6ms/step\n",
            "Epoch 42/50\n",
            "1137/1137 - 7s - loss: 0.8800 - accuracy: 0.6604 - 7s/epoch - 6ms/step\n",
            "Epoch 43/50\n",
            "1137/1137 - 7s - loss: 0.8729 - accuracy: 0.6627 - 7s/epoch - 6ms/step\n",
            "Epoch 44/50\n",
            "1137/1137 - 6s - loss: 0.8716 - accuracy: 0.6647 - 6s/epoch - 5ms/step\n",
            "Epoch 45/50\n",
            "1137/1137 - 6s - loss: 0.8697 - accuracy: 0.6649 - 6s/epoch - 6ms/step\n",
            "Epoch 46/50\n",
            "1137/1137 - 5s - loss: 0.8605 - accuracy: 0.6666 - 5s/epoch - 5ms/step\n",
            "Epoch 47/50\n",
            "1137/1137 - 6s - loss: 0.8693 - accuracy: 0.6627 - 6s/epoch - 5ms/step\n",
            "Epoch 48/50\n",
            "1137/1137 - 6s - loss: 0.8543 - accuracy: 0.6709 - 6s/epoch - 5ms/step\n",
            "Epoch 49/50\n",
            "1137/1137 - 5s - loss: 0.8537 - accuracy: 0.6706 - 5s/epoch - 5ms/step\n",
            "Epoch 50/50\n",
            "1137/1137 - 6s - loss: 0.8539 - accuracy: 0.6737 - 6s/epoch - 6ms/step\n",
            "Score for fold 4: loss of 2.2752251625061035; accuracy of 26.57865881919861%\n",
            "Training for fold 5 ...\n",
            "Epoch 1/50\n",
            "1421/1421 - 11s - loss: 1.5199 - accuracy: 0.3431 - 11s/epoch - 8ms/step\n",
            "Epoch 2/50\n",
            "1421/1421 - 6s - loss: 1.4713 - accuracy: 0.3733 - 6s/epoch - 5ms/step\n",
            "Epoch 3/50\n",
            "1421/1421 - 7s - loss: 1.4266 - accuracy: 0.3990 - 7s/epoch - 5ms/step\n",
            "Epoch 4/50\n",
            "1421/1421 - 7s - loss: 1.3838 - accuracy: 0.4230 - 7s/epoch - 5ms/step\n",
            "Epoch 5/50\n",
            "1421/1421 - 7s - loss: 1.3454 - accuracy: 0.4438 - 7s/epoch - 5ms/step\n",
            "Epoch 6/50\n",
            "1421/1421 - 7s - loss: 1.3086 - accuracy: 0.4609 - 7s/epoch - 5ms/step\n",
            "Epoch 7/50\n",
            "1421/1421 - 7s - loss: 1.2827 - accuracy: 0.4743 - 7s/epoch - 5ms/step\n",
            "Epoch 8/50\n",
            "1421/1421 - 8s - loss: 1.2561 - accuracy: 0.4871 - 8s/epoch - 5ms/step\n",
            "Epoch 9/50\n",
            "1421/1421 - 7s - loss: 1.2298 - accuracy: 0.4972 - 7s/epoch - 5ms/step\n",
            "Epoch 10/50\n",
            "1421/1421 - 7s - loss: 1.2056 - accuracy: 0.5087 - 7s/epoch - 5ms/step\n",
            "Epoch 11/50\n",
            "1421/1421 - 7s - loss: 1.1853 - accuracy: 0.5214 - 7s/epoch - 5ms/step\n",
            "Epoch 12/50\n",
            "1421/1421 - 7s - loss: 1.1646 - accuracy: 0.5300 - 7s/epoch - 5ms/step\n",
            "Epoch 13/50\n",
            "1421/1421 - 6s - loss: 1.1538 - accuracy: 0.5361 - 6s/epoch - 5ms/step\n",
            "Epoch 14/50\n",
            "1421/1421 - 7s - loss: 1.1335 - accuracy: 0.5461 - 7s/epoch - 5ms/step\n",
            "Epoch 15/50\n",
            "1421/1421 - 6s - loss: 1.1174 - accuracy: 0.5508 - 6s/epoch - 5ms/step\n",
            "Epoch 16/50\n",
            "1421/1421 - 7s - loss: 1.1013 - accuracy: 0.5620 - 7s/epoch - 5ms/step\n",
            "Epoch 17/50\n",
            "1421/1421 - 7s - loss: 1.0928 - accuracy: 0.5666 - 7s/epoch - 5ms/step\n",
            "Epoch 18/50\n",
            "1421/1421 - 7s - loss: 1.0775 - accuracy: 0.5710 - 7s/epoch - 5ms/step\n",
            "Epoch 19/50\n",
            "1421/1421 - 7s - loss: 1.0724 - accuracy: 0.5705 - 7s/epoch - 5ms/step\n",
            "Epoch 20/50\n",
            "1421/1421 - 8s - loss: 1.0620 - accuracy: 0.5780 - 8s/epoch - 5ms/step\n",
            "Epoch 21/50\n",
            "1421/1421 - 7s - loss: 1.0511 - accuracy: 0.5860 - 7s/epoch - 5ms/step\n",
            "Epoch 22/50\n",
            "1421/1421 - 7s - loss: 1.0451 - accuracy: 0.5850 - 7s/epoch - 5ms/step\n",
            "Epoch 23/50\n",
            "1421/1421 - 7s - loss: 1.0358 - accuracy: 0.5904 - 7s/epoch - 5ms/step\n",
            "Epoch 24/50\n",
            "1421/1421 - 7s - loss: 1.0292 - accuracy: 0.5939 - 7s/epoch - 5ms/step\n",
            "Epoch 25/50\n",
            "1421/1421 - 8s - loss: 1.0197 - accuracy: 0.5989 - 8s/epoch - 6ms/step\n",
            "Epoch 26/50\n",
            "1421/1421 - 7s - loss: 1.0137 - accuracy: 0.6008 - 7s/epoch - 5ms/step\n",
            "Epoch 27/50\n",
            "1421/1421 - 8s - loss: 1.0065 - accuracy: 0.6030 - 8s/epoch - 5ms/step\n",
            "Epoch 28/50\n",
            "1421/1421 - 8s - loss: 1.0002 - accuracy: 0.6054 - 8s/epoch - 6ms/step\n",
            "Epoch 29/50\n",
            "1421/1421 - 8s - loss: 0.9928 - accuracy: 0.6096 - 8s/epoch - 6ms/step\n",
            "Epoch 30/50\n",
            "1421/1421 - 7s - loss: 0.9869 - accuracy: 0.6116 - 7s/epoch - 5ms/step\n",
            "Epoch 31/50\n",
            "1421/1421 - 11s - loss: 0.9839 - accuracy: 0.6142 - 11s/epoch - 8ms/step\n",
            "Epoch 32/50\n",
            "1421/1421 - 12s - loss: 0.9710 - accuracy: 0.6194 - 12s/epoch - 8ms/step\n",
            "Epoch 33/50\n",
            "1421/1421 - 8s - loss: 0.9725 - accuracy: 0.6211 - 8s/epoch - 5ms/step\n",
            "Epoch 34/50\n",
            "1421/1421 - 7s - loss: 0.9652 - accuracy: 0.6255 - 7s/epoch - 5ms/step\n",
            "Epoch 35/50\n",
            "1421/1421 - 8s - loss: 0.9611 - accuracy: 0.6266 - 8s/epoch - 6ms/step\n",
            "Epoch 36/50\n",
            "1421/1421 - 7s - loss: 0.9554 - accuracy: 0.6272 - 7s/epoch - 5ms/step\n",
            "Epoch 37/50\n",
            "1421/1421 - 8s - loss: 0.9558 - accuracy: 0.6275 - 8s/epoch - 5ms/step\n",
            "Epoch 38/50\n",
            "1421/1421 - 7s - loss: 0.9492 - accuracy: 0.6306 - 7s/epoch - 5ms/step\n",
            "Epoch 39/50\n",
            "1421/1421 - 7s - loss: 0.9450 - accuracy: 0.6308 - 7s/epoch - 5ms/step\n",
            "Epoch 40/50\n",
            "1421/1421 - 7s - loss: 0.9416 - accuracy: 0.6333 - 7s/epoch - 5ms/step\n",
            "Epoch 41/50\n",
            "1421/1421 - 7s - loss: 0.9393 - accuracy: 0.6323 - 7s/epoch - 5ms/step\n",
            "Epoch 42/50\n",
            "1421/1421 - 8s - loss: 0.9311 - accuracy: 0.6368 - 8s/epoch - 5ms/step\n",
            "Epoch 43/50\n",
            "1421/1421 - 7s - loss: 0.9335 - accuracy: 0.6360 - 7s/epoch - 5ms/step\n",
            "Epoch 44/50\n",
            "1421/1421 - 8s - loss: 0.9283 - accuracy: 0.6386 - 8s/epoch - 6ms/step\n",
            "Epoch 45/50\n",
            "1421/1421 - 7s - loss: 0.9306 - accuracy: 0.6350 - 7s/epoch - 5ms/step\n",
            "Epoch 46/50\n",
            "1421/1421 - 8s - loss: 0.9154 - accuracy: 0.6459 - 8s/epoch - 5ms/step\n",
            "Epoch 47/50\n",
            "1421/1421 - 7s - loss: 0.9190 - accuracy: 0.6436 - 7s/epoch - 5ms/step\n",
            "Epoch 48/50\n",
            "1421/1421 - 8s - loss: 0.9166 - accuracy: 0.6446 - 8s/epoch - 5ms/step\n",
            "Epoch 49/50\n",
            "1421/1421 - 7s - loss: 0.9070 - accuracy: 0.6489 - 7s/epoch - 5ms/step\n",
            "Epoch 50/50\n",
            "1421/1421 - 7s - loss: 0.9015 - accuracy: 0.6498 - 7s/epoch - 5ms/step\n",
            "Score for fold 5: loss of 2.366286277770996; accuracy of 25.973597168922424%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results of using time-series cross-validation:\n",
        "\n",
        "**Training performance:**\n",
        "The training performance shows a decrease in loss and an increase in accuracy over the epochs. This means that the model is learning well. The model shows variability across folds, which is normal for time-series data.\n",
        "\n",
        "**Cross Validation Scores:**\n",
        "- The accuracy scores during cross-validation are relatively low (around 22-26%) This suggests that the model is not adequately capturing the complexity or patterns for accurate predictions.\n",
        "- The model shows consistency of scores across the folds which suggests the the model's performance is not overly dependent on a specific subset of data. This leads to the conclusion that the challenge lies in the complexity of the task rather than using the wrong subset."
      ],
      "metadata": {
        "id": "qjNcoVsEsToA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps:\n",
        "\n",
        "There are more steps that could be taken to improve the LSTM model. These include:\n",
        "\n",
        "1. Combining LSTM and CNN layers to accommodate for the complexity of the data\n",
        "2. Feature Engineering: Review and potentially enhance feature engineering by creating more meaningful features or transforming existing ones.\n",
        "3. Address Class Imbalance: By using techniques like class wighting, oversampling, or undersampling.\n",
        "\n",
        "\n",
        "However, for the time being, I will stop exploring the LSTM model and I will test other models on the other targets."
      ],
      "metadata": {
        "id": "WaQsymSxuPsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Model: Random forest for 28P"
      ],
      "metadata": {
        "id": "YTMs39XP1BSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "file_path = '/content/Inputs-Targets.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data['Year'] = data['Date'].dt.year\n",
        "data['Month'] = data['Date'].dt.month\n",
        "X = data.drop(['Date', '7P', '14P', '28P'], axis=1)\n",
        "y = data['28P']\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_encoded = encoder.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = rf_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1)))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaHcSXTI1IER",
        "outputId": "267f1fdd-771a-403d-89aa-b4abb25abbf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[ 196   17    1    0    0]\n",
            " [  71 2406   95    6    2]\n",
            " [ 124  111 2110  122   16]\n",
            " [ 108   11  114 1786  138]\n",
            " [  61    2   10  123 3279]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.92      0.51       214\n",
            "           1       0.94      0.93      0.94      2580\n",
            "           2       0.91      0.85      0.88      2483\n",
            "           3       0.88      0.83      0.85      2157\n",
            "           4       0.95      0.94      0.95      3475\n",
            "\n",
            "    accuracy                           0.90     10909\n",
            "   macro avg       0.81      0.89      0.82     10909\n",
            "weighted avg       0.91      0.90      0.90     10909\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis:\n",
        "\n",
        "**Confusion Matrix:**\n",
        "- Class 0: It shows a high recall of 92% but a low precision of 35%. This model is good at identifying true positives but it also incorrectly classifies instances from other classes as class 0\n",
        "- Class 1 to 4: They show higher precision and recall, which indicates better performance. Classes 1,2,3, and 4 show precision that is above 88%. The recall ranges between 83% and 94%, which means that the model is accurate for these classes.\n",
        "\n",
        "**Classification Report Analysis:**\n",
        "- As mentioned above, class 0 is the only class that shows a low F1 score. F1 scores help determine if there is a good balance between precision and recall. However, classes 1 to 4 show high F1 scores, which suggests a good balance.\n",
        "- The overall accuracy of the model is 90%, which means that the model is accurately predicting the rainfall 90% of the time. <br>\n",
        "- Macro Avg is 82% which is a good indicator of the model's performance across classes, especially if there is a class imbalance.<br>\n",
        "Weighted Avg: The weighted average for the F1 score is 90%, which is pretty high considering the classes imbalance.\n"
      ],
      "metadata": {
        "id": "Os5oc5Sz5ukj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv('/content/Inputs-Targets.csv')\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data['Year'] = data['Date'].dt.year\n",
        "data['Month'] = data['Date'].dt.month\n",
        "X = data.drop(['Date', '7P', '14P', '28P'], axis=1)\n",
        "y = data['14P']\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_encoded = encoder.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
        "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
        "model.add(Dense(y_encoded.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, validation_data=(X_test_reshaped, y_test), verbose=2)\n",
        "\n",
        "print(history.history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPR_S9H44fKp",
        "outputId": "d7642a3d-4068-41ef-f715-aee1aa0307cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1364/1364 - 7s - loss: 1.4677 - accuracy: 0.3562 - val_loss: 1.4334 - val_accuracy: 0.3730 - 7s/epoch - 5ms/step\n",
            "Epoch 2/50\n",
            "1364/1364 - 4s - loss: 1.3979 - accuracy: 0.4031 - val_loss: 1.3939 - val_accuracy: 0.3960 - 4s/epoch - 3ms/step\n",
            "Epoch 3/50\n",
            "1364/1364 - 5s - loss: 1.3495 - accuracy: 0.4281 - val_loss: 1.3448 - val_accuracy: 0.4285 - 5s/epoch - 3ms/step\n",
            "Epoch 4/50\n",
            "1364/1364 - 4s - loss: 1.3054 - accuracy: 0.4549 - val_loss: 1.3152 - val_accuracy: 0.4444 - 4s/epoch - 3ms/step\n",
            "Epoch 5/50\n",
            "1364/1364 - 4s - loss: 1.2661 - accuracy: 0.4774 - val_loss: 1.2837 - val_accuracy: 0.4620 - 4s/epoch - 3ms/step\n",
            "Epoch 6/50\n",
            "1364/1364 - 4s - loss: 1.2325 - accuracy: 0.4935 - val_loss: 1.2573 - val_accuracy: 0.4790 - 4s/epoch - 3ms/step\n",
            "Epoch 7/50\n",
            "1364/1364 - 5s - loss: 1.2017 - accuracy: 0.5084 - val_loss: 1.2375 - val_accuracy: 0.4911 - 5s/epoch - 4ms/step\n",
            "Epoch 8/50\n",
            "1364/1364 - 5s - loss: 1.1764 - accuracy: 0.5240 - val_loss: 1.2103 - val_accuracy: 0.5010 - 5s/epoch - 3ms/step\n",
            "Epoch 9/50\n",
            "1364/1364 - 6s - loss: 1.1529 - accuracy: 0.5337 - val_loss: 1.1987 - val_accuracy: 0.5115 - 6s/epoch - 4ms/step\n",
            "Epoch 10/50\n",
            "1364/1364 - 4s - loss: 1.1314 - accuracy: 0.5459 - val_loss: 1.1829 - val_accuracy: 0.5217 - 4s/epoch - 3ms/step\n",
            "Epoch 11/50\n",
            "1364/1364 - 4s - loss: 1.1130 - accuracy: 0.5546 - val_loss: 1.1687 - val_accuracy: 0.5262 - 4s/epoch - 3ms/step\n",
            "Epoch 12/50\n",
            "1364/1364 - 5s - loss: 1.0951 - accuracy: 0.5610 - val_loss: 1.1526 - val_accuracy: 0.5361 - 5s/epoch - 4ms/step\n",
            "Epoch 13/50\n",
            "1364/1364 - 4s - loss: 1.0786 - accuracy: 0.5721 - val_loss: 1.1404 - val_accuracy: 0.5441 - 4s/epoch - 3ms/step\n",
            "Epoch 14/50\n",
            "1364/1364 - 4s - loss: 1.0630 - accuracy: 0.5818 - val_loss: 1.1281 - val_accuracy: 0.5435 - 4s/epoch - 3ms/step\n",
            "Epoch 15/50\n",
            "1364/1364 - 5s - loss: 1.0485 - accuracy: 0.5887 - val_loss: 1.1141 - val_accuracy: 0.5544 - 5s/epoch - 3ms/step\n",
            "Epoch 16/50\n",
            "1364/1364 - 4s - loss: 1.0336 - accuracy: 0.5940 - val_loss: 1.0989 - val_accuracy: 0.5638 - 4s/epoch - 3ms/step\n",
            "Epoch 17/50\n",
            "1364/1364 - 4s - loss: 1.0198 - accuracy: 0.6012 - val_loss: 1.0920 - val_accuracy: 0.5631 - 4s/epoch - 3ms/step\n",
            "Epoch 18/50\n",
            "1364/1364 - 5s - loss: 1.0089 - accuracy: 0.6061 - val_loss: 1.0763 - val_accuracy: 0.5781 - 5s/epoch - 4ms/step\n",
            "Epoch 19/50\n",
            "1364/1364 - 4s - loss: 0.9950 - accuracy: 0.6118 - val_loss: 1.0743 - val_accuracy: 0.5759 - 4s/epoch - 3ms/step\n",
            "Epoch 20/50\n",
            "1364/1364 - 4s - loss: 0.9846 - accuracy: 0.6158 - val_loss: 1.0659 - val_accuracy: 0.5813 - 4s/epoch - 3ms/step\n",
            "Epoch 21/50\n",
            "1364/1364 - 5s - loss: 0.9729 - accuracy: 0.6227 - val_loss: 1.0544 - val_accuracy: 0.5867 - 5s/epoch - 4ms/step\n",
            "Epoch 22/50\n",
            "1364/1364 - 4s - loss: 0.9632 - accuracy: 0.6265 - val_loss: 1.0441 - val_accuracy: 0.5907 - 4s/epoch - 3ms/step\n",
            "Epoch 23/50\n",
            "1364/1364 - 4s - loss: 0.9526 - accuracy: 0.6326 - val_loss: 1.0364 - val_accuracy: 0.5948 - 4s/epoch - 3ms/step\n",
            "Epoch 24/50\n",
            "1364/1364 - 5s - loss: 0.9443 - accuracy: 0.6359 - val_loss: 1.0315 - val_accuracy: 0.5995 - 5s/epoch - 4ms/step\n",
            "Epoch 25/50\n",
            "1364/1364 - 4s - loss: 0.9352 - accuracy: 0.6371 - val_loss: 1.0243 - val_accuracy: 0.5969 - 4s/epoch - 3ms/step\n",
            "Epoch 26/50\n",
            "1364/1364 - 4s - loss: 0.9253 - accuracy: 0.6411 - val_loss: 1.0157 - val_accuracy: 0.6078 - 4s/epoch - 3ms/step\n",
            "Epoch 27/50\n",
            "1364/1364 - 5s - loss: 0.9180 - accuracy: 0.6457 - val_loss: 1.0078 - val_accuracy: 0.6074 - 5s/epoch - 3ms/step\n",
            "Epoch 28/50\n",
            "1364/1364 - 4s - loss: 0.9099 - accuracy: 0.6504 - val_loss: 1.0058 - val_accuracy: 0.6074 - 4s/epoch - 3ms/step\n",
            "Epoch 29/50\n",
            "1364/1364 - 4s - loss: 0.9017 - accuracy: 0.6512 - val_loss: 0.9993 - val_accuracy: 0.6117 - 4s/epoch - 3ms/step\n",
            "Epoch 30/50\n",
            "1364/1364 - 5s - loss: 0.8954 - accuracy: 0.6547 - val_loss: 0.9938 - val_accuracy: 0.6089 - 5s/epoch - 3ms/step\n",
            "Epoch 31/50\n",
            "1364/1364 - 4s - loss: 0.8873 - accuracy: 0.6590 - val_loss: 0.9851 - val_accuracy: 0.6136 - 4s/epoch - 3ms/step\n",
            "Epoch 32/50\n",
            "1364/1364 - 4s - loss: 0.8808 - accuracy: 0.6594 - val_loss: 0.9813 - val_accuracy: 0.6174 - 4s/epoch - 3ms/step\n",
            "Epoch 33/50\n",
            "1364/1364 - 5s - loss: 0.8746 - accuracy: 0.6630 - val_loss: 0.9806 - val_accuracy: 0.6231 - 5s/epoch - 3ms/step\n",
            "Epoch 34/50\n",
            "1364/1364 - 4s - loss: 0.8687 - accuracy: 0.6669 - val_loss: 0.9710 - val_accuracy: 0.6209 - 4s/epoch - 3ms/step\n",
            "Epoch 35/50\n",
            "1364/1364 - 4s - loss: 0.8622 - accuracy: 0.6676 - val_loss: 0.9724 - val_accuracy: 0.6227 - 4s/epoch - 3ms/step\n",
            "Epoch 36/50\n",
            "1364/1364 - 4s - loss: 0.8568 - accuracy: 0.6724 - val_loss: 0.9639 - val_accuracy: 0.6256 - 4s/epoch - 3ms/step\n",
            "Epoch 37/50\n",
            "1364/1364 - 5s - loss: 0.8518 - accuracy: 0.6726 - val_loss: 0.9570 - val_accuracy: 0.6248 - 5s/epoch - 4ms/step\n",
            "Epoch 38/50\n",
            "1364/1364 - 4s - loss: 0.8451 - accuracy: 0.6755 - val_loss: 0.9593 - val_accuracy: 0.6288 - 4s/epoch - 3ms/step\n",
            "Epoch 39/50\n",
            "1364/1364 - 4s - loss: 0.8414 - accuracy: 0.6773 - val_loss: 0.9545 - val_accuracy: 0.6286 - 4s/epoch - 3ms/step\n",
            "Epoch 40/50\n",
            "1364/1364 - 5s - loss: 0.8367 - accuracy: 0.6785 - val_loss: 0.9470 - val_accuracy: 0.6335 - 5s/epoch - 4ms/step\n",
            "Epoch 41/50\n",
            "1364/1364 - 4s - loss: 0.8310 - accuracy: 0.6822 - val_loss: 0.9493 - val_accuracy: 0.6267 - 4s/epoch - 3ms/step\n",
            "Epoch 42/50\n",
            "1364/1364 - 4s - loss: 0.8260 - accuracy: 0.6829 - val_loss: 0.9448 - val_accuracy: 0.6308 - 4s/epoch - 3ms/step\n",
            "Epoch 43/50\n",
            "1364/1364 - 5s - loss: 0.8220 - accuracy: 0.6851 - val_loss: 0.9359 - val_accuracy: 0.6353 - 5s/epoch - 4ms/step\n",
            "Epoch 44/50\n",
            "1364/1364 - 4s - loss: 0.8183 - accuracy: 0.6858 - val_loss: 0.9308 - val_accuracy: 0.6334 - 4s/epoch - 3ms/step\n",
            "Epoch 45/50\n",
            "1364/1364 - 4s - loss: 0.8135 - accuracy: 0.6880 - val_loss: 0.9282 - val_accuracy: 0.6411 - 4s/epoch - 3ms/step\n",
            "Epoch 46/50\n",
            "1364/1364 - 5s - loss: 0.8098 - accuracy: 0.6897 - val_loss: 0.9270 - val_accuracy: 0.6396 - 5s/epoch - 3ms/step\n",
            "Epoch 47/50\n",
            "1364/1364 - 4s - loss: 0.8067 - accuracy: 0.6906 - val_loss: 0.9276 - val_accuracy: 0.6411 - 4s/epoch - 3ms/step\n",
            "Epoch 48/50\n",
            "1364/1364 - 4s - loss: 0.8022 - accuracy: 0.6926 - val_loss: 0.9197 - val_accuracy: 0.6414 - 4s/epoch - 3ms/step\n",
            "Epoch 49/50\n",
            "1364/1364 - 5s - loss: 0.7993 - accuracy: 0.6939 - val_loss: 0.9173 - val_accuracy: 0.6447 - 5s/epoch - 3ms/step\n",
            "Epoch 50/50\n",
            "1364/1364 - 5s - loss: 0.7945 - accuracy: 0.6968 - val_loss: 0.9187 - val_accuracy: 0.6471 - 5s/epoch - 3ms/step\n",
            "{'loss': [1.4676668643951416, 1.3979322910308838, 1.3494657278060913, 1.3054057359695435, 1.266071081161499, 1.2324577569961548, 1.2016555070877075, 1.1763664484024048, 1.1528987884521484, 1.1313798427581787, 1.1130415201187134, 1.095118522644043, 1.0785996913909912, 1.0629518032073975, 1.0484607219696045, 1.0335503816604614, 1.0198192596435547, 1.008864402770996, 0.9950152635574341, 0.9846129417419434, 0.9728950262069702, 0.9631944298744202, 0.9525797367095947, 0.9443304538726807, 0.9352223873138428, 0.9253273606300354, 0.9180374145507812, 0.9098719358444214, 0.9016682505607605, 0.8953503966331482, 0.8873221278190613, 0.8807641863822937, 0.8745853304862976, 0.8686820268630981, 0.8622312545776367, 0.8568246960639954, 0.851799488067627, 0.8451313972473145, 0.8413758277893066, 0.8367290496826172, 0.830976128578186, 0.8259686231613159, 0.8219773173332214, 0.818252444267273, 0.8134580254554749, 0.8098278045654297, 0.8067241311073303, 0.8021922707557678, 0.7992627024650574, 0.7945285439491272], 'accuracy': [0.35618194937705994, 0.4031396806240082, 0.4281425476074219, 0.4548642039299011, 0.4773919880390167, 0.49352583289146423, 0.508399248123169, 0.5239830613136292, 0.5336541533470154, 0.545892059803009, 0.5545777678489685, 0.561040461063385, 0.5721324682235718, 0.5817577838897705, 0.5887475609779358, 0.5940185785293579, 0.6012146472930908, 0.6061418652534485, 0.6118024587631226, 0.615790069103241, 0.6227340698242188, 0.6265153884887695, 0.6326114535331726, 0.6359115242958069, 0.6370803117752075, 0.6411367058753967, 0.6456514000892639, 0.6504411697387695, 0.6512203216552734, 0.6547496318817139, 0.6589893698692322, 0.6594018340110779, 0.6630228161811829, 0.666895866394043, 0.6675604581832886, 0.6724189519882202, 0.672556459903717, 0.6755127906799316, 0.6773003339767456, 0.6785378456115723, 0.6822046637535095, 0.6828922033309937, 0.6851151585578918, 0.6857797503471375, 0.6880486011505127, 0.6896757483482361, 0.6905924081802368, 0.6926091313362122, 0.6939154267311096, 0.6967800855636597], 'val_loss': [1.4334266185760498, 1.3938772678375244, 1.3448406457901, 1.3151741027832031, 1.2837395668029785, 1.2573349475860596, 1.2375291585922241, 1.2102783918380737, 1.198685646057129, 1.182881236076355, 1.1686644554138184, 1.1526365280151367, 1.140440821647644, 1.1280529499053955, 1.1140568256378174, 1.098883867263794, 1.0920054912567139, 1.0763028860092163, 1.0742871761322021, 1.0658842325210571, 1.054375171661377, 1.0441269874572754, 1.0363889932632446, 1.0314934253692627, 1.024271011352539, 1.0156563520431519, 1.0078281164169312, 1.0058343410491943, 0.9992915391921997, 0.9938057065010071, 0.9851199984550476, 0.9812607169151306, 0.9805757999420166, 0.9710302352905273, 0.9723703265190125, 0.9638929963111877, 0.9569708108901978, 0.959251880645752, 0.9545189738273621, 0.9470034241676331, 0.9493122696876526, 0.9448484182357788, 0.9358925223350525, 0.9308226704597473, 0.9281920790672302, 0.9269940257072449, 0.9276480674743652, 0.9196511507034302, 0.9172688722610474, 0.918662965297699], 'val_accuracy': [0.3729947805404663, 0.3960033059120178, 0.42845356464385986, 0.4444037079811096, 0.4620038568973541, 0.47896233201026917, 0.4910624325275421, 0.500962495803833, 0.5115042328834534, 0.5216793417930603, 0.5261710286140442, 0.5360711216926575, 0.5441378951072693, 0.5434961915016174, 0.554404616355896, 0.5638463497161865, 0.5631130337715149, 0.5780548453330994, 0.5759464502334595, 0.5812631845474243, 0.5866715312004089, 0.5907049179077148, 0.5948299765586853, 0.5995050072669983, 0.5969383120536804, 0.607846736907959, 0.6073883771896362, 0.6073883771896362, 0.6116967797279358, 0.6089467406272888, 0.6136217713356018, 0.6173801422119141, 0.6230635046958923, 0.6208634972572327, 0.6226968765258789, 0.6256301999092102, 0.624805212020874, 0.6288385987281799, 0.6285635828971863, 0.6335136294364929, 0.62673020362854, 0.630763590335846, 0.6352552771568298, 0.6334219574928284, 0.6411219835281372, 0.6395636796951294, 0.6411219835281372, 0.6413969993591309, 0.6446970105171204, 0.6470804214477539]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis:\n",
        "Training and Validation Loss:\n",
        "- The validation and training loss decrease over 50 epochs, which is a good sign.\n",
        "- The loss started at 1.469 and ended at 0.7947, while the validation loss started at 1.4377 and ended at 0.9061. The consistent decrease suggests that the model is generalizing well without an obvious overfitting.\n",
        "\n",
        "Training and Validation Accuracy:\n",
        "- The training accuracy started at 35.18% and increased to 96.61%, while validation accuracy started at 36.78% and stopped at 64.96%. The higher training accuracy compared to validation accuracy is normal. In general, the model shows a good level of generalization taking into account the nature of the task.\n",
        "\n",
        "Potential Overfitting: The last epochs show a slightly higher gap between the training accuracy and the validation accuracy. This could be classified as an early sign of overfitting."
      ],
      "metadata": {
        "id": "ZBRDCtQa9SRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Steps:**\n",
        "\n",
        "1. Experiment with different hyperparamters\n",
        "2. Implement Early Stopping\n"
      ],
      "metadata": {
        "id": "hsvPJ4HI_CwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "data = pd.read_csv('/content/Inputs-Targets.csv')\n",
        "\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data['Year'] = data['Date'].dt.year\n",
        "data['Month'] = data['Date'].dt.month\n",
        "X = data.drop(['Date', '7P', '14P', '28P'], axis=1)\n",
        "y = data['14P']\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_encoded = encoder.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
        "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(70, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(GRU(50))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y_encoded.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train_reshaped, y_train, validation_data=(X_test_reshaped, y_test),\n",
        "                    epochs=50, batch_size=64, callbacks=[early_stopping], verbose=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGMI-xT1_Gf1",
        "outputId": "b336ea8f-8a3d-47c4-dd74-05ccc451070c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "682/682 - 10s - loss: 1.4870 - accuracy: 0.3392 - val_loss: 1.4517 - val_accuracy: 0.3571 - 10s/epoch - 15ms/step\n",
            "Epoch 2/50\n",
            "682/682 - 4s - loss: 1.4376 - accuracy: 0.3719 - val_loss: 1.4177 - val_accuracy: 0.3757 - 4s/epoch - 5ms/step\n",
            "Epoch 3/50\n",
            "682/682 - 5s - loss: 1.4077 - accuracy: 0.3905 - val_loss: 1.3819 - val_accuracy: 0.4065 - 5s/epoch - 7ms/step\n",
            "Epoch 4/50\n",
            "682/682 - 4s - loss: 1.3764 - accuracy: 0.4056 - val_loss: 1.3510 - val_accuracy: 0.4252 - 4s/epoch - 6ms/step\n",
            "Epoch 5/50\n",
            "682/682 - 4s - loss: 1.3458 - accuracy: 0.4249 - val_loss: 1.3129 - val_accuracy: 0.4385 - 4s/epoch - 6ms/step\n",
            "Epoch 6/50\n",
            "682/682 - 6s - loss: 1.3199 - accuracy: 0.4388 - val_loss: 1.2810 - val_accuracy: 0.4612 - 6s/epoch - 8ms/step\n",
            "Epoch 7/50\n",
            "682/682 - 4s - loss: 1.2956 - accuracy: 0.4513 - val_loss: 1.2498 - val_accuracy: 0.4758 - 4s/epoch - 5ms/step\n",
            "Epoch 8/50\n",
            "682/682 - 4s - loss: 1.2692 - accuracy: 0.4643 - val_loss: 1.2248 - val_accuracy: 0.4883 - 4s/epoch - 5ms/step\n",
            "Epoch 9/50\n",
            "682/682 - 4s - loss: 1.2489 - accuracy: 0.4771 - val_loss: 1.1974 - val_accuracy: 0.5043 - 4s/epoch - 6ms/step\n",
            "Epoch 10/50\n",
            "682/682 - 4s - loss: 1.2287 - accuracy: 0.4852 - val_loss: 1.1713 - val_accuracy: 0.5187 - 4s/epoch - 6ms/step\n",
            "Epoch 11/50\n",
            "682/682 - 4s - loss: 1.2095 - accuracy: 0.4961 - val_loss: 1.1468 - val_accuracy: 0.5324 - 4s/epoch - 5ms/step\n",
            "Epoch 12/50\n",
            "682/682 - 4s - loss: 1.1934 - accuracy: 0.5048 - val_loss: 1.1298 - val_accuracy: 0.5455 - 4s/epoch - 5ms/step\n",
            "Epoch 13/50\n",
            "682/682 - 5s - loss: 1.1833 - accuracy: 0.5097 - val_loss: 1.1133 - val_accuracy: 0.5531 - 5s/epoch - 7ms/step\n",
            "Epoch 14/50\n",
            "682/682 - 4s - loss: 1.1678 - accuracy: 0.5199 - val_loss: 1.0922 - val_accuracy: 0.5614 - 4s/epoch - 5ms/step\n",
            "Epoch 15/50\n",
            "682/682 - 4s - loss: 1.1557 - accuracy: 0.5244 - val_loss: 1.0767 - val_accuracy: 0.5706 - 4s/epoch - 5ms/step\n",
            "Epoch 16/50\n",
            "682/682 - 5s - loss: 1.1422 - accuracy: 0.5293 - val_loss: 1.0613 - val_accuracy: 0.5792 - 5s/epoch - 7ms/step\n",
            "Epoch 17/50\n",
            "682/682 - 4s - loss: 1.1288 - accuracy: 0.5392 - val_loss: 1.0453 - val_accuracy: 0.5867 - 4s/epoch - 6ms/step\n",
            "Epoch 18/50\n",
            "682/682 - 4s - loss: 1.1195 - accuracy: 0.5381 - val_loss: 1.0367 - val_accuracy: 0.5917 - 4s/epoch - 6ms/step\n",
            "Epoch 19/50\n",
            "682/682 - 4s - loss: 1.1141 - accuracy: 0.5458 - val_loss: 1.0212 - val_accuracy: 0.6006 - 4s/epoch - 7ms/step\n",
            "Epoch 20/50\n",
            "682/682 - 4s - loss: 1.0985 - accuracy: 0.5509 - val_loss: 1.0101 - val_accuracy: 0.6057 - 4s/epoch - 6ms/step\n",
            "Epoch 21/50\n",
            "682/682 - 4s - loss: 1.0917 - accuracy: 0.5548 - val_loss: 0.9955 - val_accuracy: 0.6187 - 4s/epoch - 6ms/step\n",
            "Epoch 22/50\n",
            "682/682 - 4s - loss: 1.0824 - accuracy: 0.5585 - val_loss: 0.9855 - val_accuracy: 0.6187 - 4s/epoch - 6ms/step\n",
            "Epoch 23/50\n",
            "682/682 - 5s - loss: 1.0736 - accuracy: 0.5612 - val_loss: 0.9760 - val_accuracy: 0.6252 - 5s/epoch - 7ms/step\n",
            "Epoch 24/50\n",
            "682/682 - 4s - loss: 1.0642 - accuracy: 0.5673 - val_loss: 0.9655 - val_accuracy: 0.6310 - 4s/epoch - 6ms/step\n",
            "Epoch 25/50\n",
            "682/682 - 4s - loss: 1.0636 - accuracy: 0.5663 - val_loss: 0.9576 - val_accuracy: 0.6341 - 4s/epoch - 6ms/step\n",
            "Epoch 26/50\n",
            "682/682 - 5s - loss: 1.0578 - accuracy: 0.5713 - val_loss: 0.9487 - val_accuracy: 0.6385 - 5s/epoch - 7ms/step\n",
            "Epoch 27/50\n",
            "682/682 - 4s - loss: 1.0449 - accuracy: 0.5772 - val_loss: 0.9398 - val_accuracy: 0.6424 - 4s/epoch - 5ms/step\n",
            "Epoch 28/50\n",
            "682/682 - 4s - loss: 1.0407 - accuracy: 0.5791 - val_loss: 0.9318 - val_accuracy: 0.6441 - 4s/epoch - 6ms/step\n",
            "Epoch 29/50\n",
            "682/682 - 5s - loss: 1.0290 - accuracy: 0.5837 - val_loss: 0.9244 - val_accuracy: 0.6502 - 5s/epoch - 7ms/step\n",
            "Epoch 30/50\n",
            "682/682 - 4s - loss: 1.0266 - accuracy: 0.5832 - val_loss: 0.9161 - val_accuracy: 0.6530 - 4s/epoch - 5ms/step\n",
            "Epoch 31/50\n",
            "682/682 - 4s - loss: 1.0220 - accuracy: 0.5865 - val_loss: 0.9090 - val_accuracy: 0.6543 - 4s/epoch - 6ms/step\n",
            "Epoch 32/50\n",
            "682/682 - 5s - loss: 1.0176 - accuracy: 0.5878 - val_loss: 0.9010 - val_accuracy: 0.6607 - 5s/epoch - 8ms/step\n",
            "Epoch 33/50\n",
            "682/682 - 4s - loss: 1.0099 - accuracy: 0.5953 - val_loss: 0.8936 - val_accuracy: 0.6653 - 4s/epoch - 6ms/step\n",
            "Epoch 34/50\n",
            "682/682 - 4s - loss: 1.0093 - accuracy: 0.5925 - val_loss: 0.8888 - val_accuracy: 0.6670 - 4s/epoch - 5ms/step\n",
            "Epoch 35/50\n",
            "682/682 - 5s - loss: 1.0043 - accuracy: 0.5934 - val_loss: 0.8842 - val_accuracy: 0.6670 - 5s/epoch - 7ms/step\n",
            "Epoch 36/50\n",
            "682/682 - 4s - loss: 0.9976 - accuracy: 0.5968 - val_loss: 0.8745 - val_accuracy: 0.6725 - 4s/epoch - 6ms/step\n",
            "Epoch 37/50\n",
            "682/682 - 4s - loss: 0.9920 - accuracy: 0.6007 - val_loss: 0.8725 - val_accuracy: 0.6741 - 4s/epoch - 6ms/step\n",
            "Epoch 38/50\n",
            "682/682 - 5s - loss: 0.9903 - accuracy: 0.6010 - val_loss: 0.8641 - val_accuracy: 0.6738 - 5s/epoch - 7ms/step\n",
            "Epoch 39/50\n",
            "682/682 - 4s - loss: 0.9896 - accuracy: 0.5996 - val_loss: 0.8614 - val_accuracy: 0.6782 - 4s/epoch - 6ms/step\n",
            "Epoch 40/50\n",
            "682/682 - 4s - loss: 0.9799 - accuracy: 0.6068 - val_loss: 0.8541 - val_accuracy: 0.6793 - 4s/epoch - 6ms/step\n",
            "Epoch 41/50\n",
            "682/682 - 5s - loss: 0.9788 - accuracy: 0.6074 - val_loss: 0.8522 - val_accuracy: 0.6792 - 5s/epoch - 7ms/step\n",
            "Epoch 42/50\n",
            "682/682 - 4s - loss: 0.9705 - accuracy: 0.6120 - val_loss: 0.8454 - val_accuracy: 0.6802 - 4s/epoch - 6ms/step\n",
            "Epoch 43/50\n",
            "682/682 - 4s - loss: 0.9738 - accuracy: 0.6074 - val_loss: 0.8398 - val_accuracy: 0.6850 - 4s/epoch - 6ms/step\n",
            "Epoch 44/50\n",
            "682/682 - 4s - loss: 0.9691 - accuracy: 0.6100 - val_loss: 0.8367 - val_accuracy: 0.6888 - 4s/epoch - 6ms/step\n",
            "Epoch 45/50\n",
            "682/682 - 4s - loss: 0.9650 - accuracy: 0.6118 - val_loss: 0.8305 - val_accuracy: 0.6926 - 4s/epoch - 6ms/step\n",
            "Epoch 46/50\n",
            "682/682 - 4s - loss: 0.9629 - accuracy: 0.6119 - val_loss: 0.8286 - val_accuracy: 0.6905 - 4s/epoch - 5ms/step\n",
            "Epoch 47/50\n",
            "682/682 - 4s - loss: 0.9596 - accuracy: 0.6148 - val_loss: 0.8223 - val_accuracy: 0.6930 - 4s/epoch - 6ms/step\n",
            "Epoch 48/50\n",
            "682/682 - 5s - loss: 0.9587 - accuracy: 0.6168 - val_loss: 0.8207 - val_accuracy: 0.6970 - 5s/epoch - 7ms/step\n",
            "Epoch 49/50\n",
            "682/682 - 4s - loss: 0.9537 - accuracy: 0.6179 - val_loss: 0.8190 - val_accuracy: 0.6928 - 4s/epoch - 6ms/step\n",
            "Epoch 50/50\n",
            "682/682 - 4s - loss: 0.9479 - accuracy: 0.6200 - val_loss: 0.8163 - val_accuracy: 0.6922 - 4s/epoch - 6ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In-depth Analysis of Rainfall Prediction Models:**\n",
        "\n",
        "This report aims to analyze the performance of three distinct models- LSTM, Random Forest, and GRU - each model is applied to predict rainfall over time horizons (7, 14, 28) days. <br>\n",
        "\n",
        "1. LSTM on 7-day Rainfall Prediction (7P):\n",
        "  - The model showed gradual improvement over epochs which shows a learning trend\n",
        "  - The accuracy plateaued, which means that the model might be underfitting or needs more complex network architecture\n",
        "\n",
        "  I explored the LSTM model the most, and I tested the most modifications on it. In general, it performed well.\n",
        "\n",
        "\n",
        "2. Random Forest on 28-Day Rainfall Prediction (28P):\n",
        "  - This method of using ensembled provides valuable information without needing to use transformations required by neural networks.\n",
        "  - High precision and recall scores were achieved, specifically in predicting majority classes, which is an indicator of its efficiency in handling imbalanced data.\n",
        "  - Despite it being a strong, simple model, it is limited in capturing temporal dependencies, which are of importance for time-series forecasting.\n",
        "\n",
        "3. GRU on 14-Day Rainfall Prediction (14P):\n",
        "  - It is similar to the LSTM model but more efficient. I used early stopping and optimization of the hyperparameters to enhance performance.\n",
        "  - It demonstrated the highest validation accuracy among all models, which suggests effectiveness for this task\n",
        "  - Early stopping was successful in preventing overfitting\n",
        "\n",
        "\n",
        "**Comparative Overview:** <br>\n",
        "**Accuracy and Efficiency:** GRU achieved the highest accuracy. Random Forest, while less accurate, was easier and more efficient to implement. <br>\n",
        "**Model Complexity:** LSTM and GRU are naturally more complex and they require careful tuning. Random Forest is a more straightforward approach. <br>\n",
        "**Computational Resources:** LSTM and GRU require greater computational resources and time. <br>\n",
        "**Overfitting:** Early stopping was effective in preventing overfitting using the GRU model. Random Forest avoids overfitting naturally. However, LSTM showed potential overfitting. <br>\n",
        "\n",
        "**Conclusion:**<br>\n",
        "Depending on the task that is needed from predicting the rainfall, GRU or Random Forest could be used as efficient models. For future advancements, hybrid models could be used to improve the performance.\n"
      ],
      "metadata": {
        "id": "oOITGl8jBNhV"
      }
    }
  ]
}