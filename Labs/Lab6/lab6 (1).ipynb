{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB6GtBP6nUiu"
      },
      "source": [
        "<a\n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cksgAH12XRjV"
      },
      "source": [
        "# Lab 6: Sequence-to-sequence models\n",
        "\n",
        "### Description:\n",
        "For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n",
        "\n",
        "This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n",
        "\n",
        "### Deliverable:\n",
        "- Fill in the code for the RNN (using PyTorch's built-in GRU).\n",
        "- Fill in the training loop\n",
        "- Fill in the evaluation loop. In this loop, rather than using a validation set, you will sample text from the RNN.\n",
        "- Implement your own GRU cell.\n",
        "- Train your RNN on a new domain of text (Star Wars, political speeches, etc. - have fun!)\n",
        "\n",
        "### Grading Standards:\n",
        "- 20% Implementation the RNN\n",
        "- 20% Implementation training loop\n",
        "- 20% Implementation of evaluation loop\n",
        "- 20% Implementation of your own GRU cell\n",
        "- 20% Training of your RNN on a domain of your choice\n",
        "\n",
        "### Tips:\n",
        "- Read through all the helper functions, run them, and make sure you understand what they are doing\n",
        "- At each stage, ask yourself: What should the dimensions of this tensor be? Should its data type be float or int? (int is called `long` in PyTorch)\n",
        "- Don't apply a softmax inside the RNN if you are using an nn.CrossEntropyLoss (this module already applies a softmax to its input).\n",
        "\n",
        "### Example Output:\n",
        "An example of my final samples are shown below (more detail in the\n",
        "final section of this writeup), after 150 passes through the data.\n",
        "Please generate about 15 samples for each dataset.\n",
        "\n",
        "<code>\n",
        "And ifte thin forgision forward thene over up to a fear not your\n",
        "And freitions, which is great God. Behold these are the loss sub\n",
        "And ache with the Lord hath bloes, which was done to the holy Gr\n",
        "And appeicis arm vinimonahites strong in name, to doth piseling\n",
        "And miniquithers these words, he commanded order not; neither sa\n",
        "And min for many would happine even to the earth, to said unto m\n",
        "And mie first be traditions? Behold, you, because it was a sound\n",
        "And from tike ended the Lamanites had administered, and I say bi\n",
        "</code>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2i_QpSsWG4c"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 0: Readings, data loading, and high level training\n",
        "\n",
        "---\n",
        "\n",
        "There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n",
        "\n",
        "* Read the following\n",
        "\n",
        "> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) (Take note that you will not be implementing the encoder part of this tutorial.)\n",
        "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7bdZWxvJrsx",
        "outputId": "2c80e761-b166-47d9-d7bf-60c46001d77a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-15 04:21:14--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
            "Resolving piazza.com (piazza.com)... 52.1.77.241, 3.223.24.172, 34.235.220.249, ...\n",
            "Connecting to piazza.com (piazza.com)|52.1.77.241|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
            "--2023-10-15 04:21:15--  https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
            "Resolving cdn-uploads.piazza.com (cdn-uploads.piazza.com)... 108.138.246.45, 108.138.246.41, 108.138.246.58, ...\n",
            "Connecting to cdn-uploads.piazza.com (cdn-uploads.piazza.com)|108.138.246.45|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1533290 (1.5M) [application/x-gzip]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "./text_files.tar.gz 100%[===================>]   1.46M  1.66MB/s    in 0.9s    \n",
            "\n",
            "2023-10-15 04:21:16 (1.66 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
            "\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "file_len = 2579888\n"
          ]
        }
      ],
      "source": [
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz'\n",
        "! tar -xzf text_files.tar.gz\n",
        "! pip install unidecode\n",
        "! pip install torch\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "import pdb\n",
        "\n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxBeKeNjJ0NQ",
        "outputId": "a614ec71-440c-4f19-e95d-331b26e813f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " indeed more \n",
            "than \n",
            "\n",
            "enough, for it was not comfortable lore. Tom's words laid bare the hearts of \n",
            "trees and their thoughts, which were often dark and strange, and filled with \n",
            "a hatred of things that \n"
          ]
        }
      ],
      "source": [
        "chunk_len = 200\n",
        "\n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "\n",
        "print(random_chunk())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On0_WitWJ99e",
        "outputId": "7ce20c6d-17ee-4118-90fe-bb5dddc13a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "      tensor[c] = all_characters.index(string[c])\n",
        "  return tensor\n",
        "\n",
        "print(char_tensor('abcDEF'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYJPTLcaYmfI"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Creating your own GRU cell\n",
        "\n",
        "**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n",
        "\n",
        "---\n",
        "\n",
        "The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n",
        "\n",
        "Please try not to look at the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n",
        "\n",
        "**TODO:**\n",
        "* Create a custom GRU cell\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "aavAv50ZKQ-F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    super(GRU, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.W_ir = nn.Linear(input_size, hidden_size)\n",
        "    self.W_hr = nn.Linear(hidden_size, hidden_size)\n",
        "    self.W_iz = nn.Linear(input_size, hidden_size)\n",
        "    self.W_hz = nn.Linear(hidden_size, hidden_size)\n",
        "    self.W_in = nn.Linear(input_size, hidden_size)\n",
        "    self.W_hn = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.tanh = nn.Tanh()\n",
        "\n",
        "\n",
        "  def forward(self, inputs, hidden):\n",
        "    # Each layer does the following:\n",
        "    # r_t = sigmoid(W_ir*x_t + b_ir + W_hr*h_(t-1) + b_hr)\n",
        "    # z_t = sigmoid(W_iz*x_t + b_iz + W_hz*h_(t-1) + b_hz)\n",
        "    # n_t = tanh(W_in*x_t + b_in + r_t**(W_hn*h_(t-1) + b_hn))\n",
        "    # h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n",
        "    # Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n",
        "    r_t = self.sigmoid(self.W_ir(inputs) + self.W_hr(hidden))\n",
        "    z_t = self.sigmoid(self.W_iz(inputs) + self.W_hz(hidden))\n",
        "    n_t = self.tanh(self.W_in(inputs) + r_t*self.W_hn(hidden))\n",
        "    outputs = n_t\n",
        "    hiddens = (1 - z_t) * n_t + z_t * hidden\n",
        "    return outputs, hiddens ##CHECK\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtXdX-B_WiAY"
      },
      "source": [
        "---\n",
        "\n",
        "##  Part 1: Building a sequence to sequence model\n",
        "\n",
        "---\n",
        "\n",
        "Great! We have the data in a useable form. We can switch out which text file we are reading from, and trying to simulate.\n",
        "\n",
        "We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n",
        "\n",
        "\n",
        "**TODO:**\n",
        "* Create an RNN class that extends from nn.Module.\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    # more stuff here...\n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    self.gru = GRU(hidden_size, hidden_size, n_layers)\n",
        "    # self.relu = nn.ReLU()\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "  def forward(self, input_char, hidden):\n",
        "    # by reviewing the documentation, construct a forward function that properly uses the output\n",
        "    # of the GRU\n",
        "\n",
        "    output = self.embedding(input_char).view(1, 1, -1)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    output = self.out(output[0])\n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hidden_size)"
      ],
      "metadata": {
        "id": "ojaivPIa5I5U"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "hrhXghEPKD-5"
      },
      "outputs": [],
      "source": [
        "def random_training_set():\n",
        "  chunk = random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpiGObbBX0Mr"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "We now want to be able to train our network, and sample text after training.\n",
        "\n",
        "This function outlines how training a sequence style network goes.\n",
        "\n",
        "**TODO:**\n",
        "* Fill in the pieces.\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "2ALC3Pf8Kbsi"
      },
      "outputs": [],
      "source": [
        "# NOTE: decoder_optimizer, decoder, and criterion will be defined below as global variables\n",
        "def train(inp, target):\n",
        "  ## initialize hidden layers, set up gradient and loss\n",
        "    # your code here\n",
        "  ## /\n",
        "\n",
        "  decoder_optimizer.zero_grad()\n",
        "  hidden = decoder.init_hidden()\n",
        "  loss = 0\n",
        "\n",
        "  # more stuff here...\n",
        "\n",
        "  for inp_char, target_char in zip(inp, target):\n",
        "    output, hidden = decoder.forward(inp_char, hidden)\n",
        "    loss += criterion(output, target_char.unsqueeze(0))\n",
        "\n",
        "  loss.backward()\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  return loss.item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN06NUu3YRlz"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n",
        "\n",
        "If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder`then this will probabilistically sample a sequence of length `predict_len`\n",
        "\n",
        "**TODO:**\n",
        "* Fill out the evaluate function to generate text frome a primed string\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "B-bp-OZ1KjNh"
      },
      "outputs": [],
      "source": [
        "def sample_outputs(output, temperature):\n",
        "    \"\"\"Takes in a vector of unnormalized probability weights and samples a character from the distribution\"\"\"\n",
        "    return torch.multinomial(torch.exp(output / temperature), 1)\n",
        "\n",
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "  ## initialize hidden state, initialize other useful variables\n",
        "\n",
        "  hidden = decoder.init_hidden()\n",
        "  prime_input = char_tensor(prime_str)\n",
        "  predicted = prime_str\n",
        "\n",
        "  for i in range(len(prime_str) - 1):\n",
        "    output, hidden = decoder(prime_input[i], hidden)\n",
        "\n",
        "  # last character as a start for generating\n",
        "  inp = prime_input[-1]\n",
        "\n",
        "  for p in range(predict_len):\n",
        "    output, hidden = decoder(inp, hidden)\n",
        "    out_distribution = output.data.view(-1).div(temperature).exp()\n",
        "    out = torch.multinomial(out_distribution, 1)[0]\n",
        "    predicted_char = all_characters[out]\n",
        "    predicted += predicted_char\n",
        "    inp = char_tensor(predicted_char)\n",
        "\n",
        "  return predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du4AGA8PcFEW"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: (Create a GRU cell, requirements above)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFS2bpHSZEU6"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Part 5: Run it and generate some text!\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**TODO:**\n",
        "* Create some cool output\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs. These are the results, along with the prime string:\n",
        "\n",
        "---\n",
        "\n",
        " G:\n",
        "\n",
        " Gandalf was decrond.\n",
        "'All have lord you. Forward the road at least walk this is stuff, and\n",
        "went to the long grey housel-winding and kindled side was a sleep pleasuring, I do long\n",
        "row hrough. In  \n",
        "\n",
        " lo:\n",
        "\n",
        " lost death it.\n",
        "'The last of the gatherings and take you,' said Aragorn, shining out of the Gate.\n",
        "'Yes, as you there were remembaused to seen their pass, when? What\n",
        "said here, such seven an the sear\n",
        "\n",
        " lo:\n",
        "\n",
        " low, and frod to keepn\n",
        "Came of their most. But here priced doubtless to an Sam up is\n",
        "masters; he left hor as they are looked. And he could now the long to stout in the right fro horseless of\n",
        "the like\n",
        "\n",
        " I:\n",
        "\n",
        " I had been the\n",
        "in his eyes with the perushed to lest, if then only the ring and the legended\n",
        "of the less of the long they which as the\n",
        "enders of Orcovered and smood, and the p\n",
        "\n",
        " I:\n",
        "\n",
        " I they were not the lord of the hoomes.\n",
        "Home already well from the Elves. And he sat strength, and we\n",
        "housed out of the good of the days to the mountains from his perith.\n",
        "\n",
        "'Yess! Where though as if  \n",
        "\n",
        " Th:\n",
        "\n",
        " There yarden\n",
        "you would guard the hoor might. Far and then may was\n",
        "croties, too began to see the drumbred many line\n",
        "and was then hoard walk and they heart, and the chair of the\n",
        "Ents of way, might was\n",
        "\n",
        " G:\n",
        "\n",
        " Gandalf\n",
        "been lat of less the round of the stump; both and seemed to the trees and perished they\n",
        "lay are speered the less; and the wind the steep and have to she\n",
        "precious. There was in the oonly went\n",
        "\n",
        " wh:\n",
        "\n",
        " which went out of the door.\n",
        "Hull the King and of the The days of his brodo\n",
        "stumbler of the windard was a thing there, then it been shining langing\n",
        "to him poor land. They hands; though they seemed ou\n",
        "\n",
        " ra:\n",
        "\n",
        " rather,' have all the least deather\n",
        "down of the truven beginning to the house of sunk.\n",
        "'Nark shorts of the Eyes of the Gate your great nothing as Eret.\n",
        "'I wander trust horn, and there were not, it  \n",
        "\n",
        " I:\n",
        "\n",
        " I can have no mind\n",
        "together! Where don't may had one may little blung\n",
        "terrible to tales. And turn and Gandalf shall be not to as only the Cattring\n",
        "not stopped great the out them forms. On they she lo\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "-nXFeCmdKodw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "n_epochs = 5000\n",
        "print_every = 200\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 3\n",
        "lr = 0.001\n",
        "\n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKfozqw-6eqb",
        "outputId": "407473cb-7d5b-4247-ea63-00151b57fedc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[79.76727533340454 (200 10%) 406.3292]\n",
            "Whow hamerneing of wis \n",
            "in ningher I nil to hat dandarneling \n",
            "\n",
            "where if to nor now horster las of e, h \n",
            "\n",
            "[137.54473686218262 (400 20%) 353.4745]\n",
            "Wher a the foringed to \n",
            "sile to that he baming stawaser they now the a the shad at lear a knowh with t \n",
            "\n",
            "[194.69581770896912 (600 30%) 413.1023]\n",
            "Whing and \n",
            "hake \n",
            "agatrank in past be as \n",
            "lose \n",
            "sten brreaked we deador, \n",
            "this \n",
            "hads while the busted a \n",
            "\n",
            "[251.9497730731964 (800 40%) 322.5259]\n",
            "Whery \n",
            "now fame to come this look the wandor and said Githand. And mise at that he saipince the Mord a \n",
            "\n",
            "[308.868221282959 (1000 50%) 393.8513]\n",
            "Whe said \n",
            "\n",
            "\n",
            "\n",
            "The winge of the shade you sas fill of \n",
            "the ker sbed not stay \n",
            "\n",
            "\n",
            "hobbitting the ptaldenth \n",
            "\n",
            "[365.97027015686035 (1200 60%) 305.5787]\n",
            "Where we loffin. Kight If for the lave \n",
            "the Frodo was everes cawe, \n",
            "sten his he to sand in bence the \n",
            " \n",
            "\n",
            "[422.70978903770447 (1400 70%) 344.6206]\n",
            "Whor bring \n",
            "Colver pearing couthald and coulder been fell shadren that he are the cadow? The emence de \n",
            "\n",
            "[480.11427450180054 (1600 80%) 303.8893]\n",
            "Whan the like as of the day, my fill the would harre.' \n",
            "\n",
            "'Your! He \n",
            "did to the \n",
            "last and there strodge \n",
            "\n",
            "[536.6028778553009 (1800 90%) 330.9796]\n",
            "Wh them the blanger \n",
            "they was at should at was hill gair now you have look the Segaling Sam that twill \n",
            "\n",
            "[594.4218916893005 (2000 100%) 300.3323]\n",
            "Whince out \n",
            "one furching of the dimbut that the is pose butther drair, \n",
            "eam-longly out houned and on t \n",
            "\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 2000\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())\n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee0so6aKJ5L8",
        "outputId": "1ddc24d9-2668-4388-8ff1-0d054b7a113e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ca\n",
            " came \n",
            "or among and the Sam stepping. It there shragg a pale's in had shouenthing \n",
            "meabor, said Frodo were streapled to while to \n",
            "alls for may sadd, enemon of the lated and were and the Wasters in the \n",
            "p \n",
            "\n",
            " wh\n",
            " when somen in stone somethers to fit of the gnow was was not very and our fway, and the donger for the Barussed the blage with to peating and \n",
            "it sunther \n",
            "were while on at the swetten in the king. His c \n",
            "\n",
            " ra\n",
            " rast \n",
            "looks me with was foon, and goter back to notter was and \n",
            "enowers fally gone. \n",
            "\n",
            "'Yearning and the sew countion should go on \n",
            "no spents trouned of were Norning \n",
            "and by of the \n",
            "ever on the treet, an \n",
            "\n",
            " ca\n",
            " cantoo the back and anwithour musted the. But was the day at he \n",
            "were should. \n",
            "\n",
            "'It someting and leather. We said Aragorn and greaddy but \n",
            "closed for \n",
            "to \n",
            "heard little shamber the gowly here, and night  \n",
            "\n",
            " lo\n",
            " loses from I last, men the chilling. \n",
            "\n",
            "Amout the ene \n",
            "stact is anter it swen part what far as was they cobeather. \n",
            "\n",
            "Streath.' \n",
            "\n",
            "'Yes, dows them. Will over many mister side many was stange of in the Ring \n",
            "\n",
            " lo\n",
            " long and showed the Gandalf from the \n",
            "ill, and it with sistand hole for he eartion, and came it thear and mounter, and not choued upentled the plast to the end was stund to the otter things at \n",
            "the \n",
            "har \n",
            "\n",
            " lo\n",
            " lon's severt. \n",
            "\n",
            "'I's not parsed ' \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "O same as \n",
            "last of the Wioplen so steed, and the other; for \n",
            "the fare from \n",
            "Gould but while of to gail the wild said in the \n",
            "appoon the had wanter and that bee \n",
            "\n",
            " he\n",
            " he stone outsed they Dandy, now sunts me what was if \n",
            "answeep. And the that not his ofratr. And he ever tways and strange with \n",
            "it any seemed, and sheered \n",
            "all drew out a heart answerbed will king his e \n",
            "\n",
            " wh\n",
            " where to in the \n",
            "know not file name \n",
            "store and the blady sung eyed, put in the spetther. \n",
            "but have was the clight; or seem. But But in the curd night no leadly, and you the kind, and with the ever, unto \n",
            "\n",
            " lo\n",
            " long agguled. \n",
            "\n",
            "'Yen supple \n",
            "the blacking lay, your had not parting thering my bear further. But is preads this peared at leavion to the yoused \n",
            "been the greater I caused to that were \n",
            "bripping of feett \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJhgDc2IauPE"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: Generate output on a different dataset\n",
        "\n",
        "---\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle\n",
        "\n",
        "* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jovian opendatasets --upgrade --quiet\n",
        "!pip install -q kaggle\n",
        "import urllib.request\n",
        "import opendatasets as od\n",
        "import pandas\n",
        "\n",
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/ishikajohari/taylor-swift-all-lyrics-30-albums/data\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0Mae6xYRUPP",
        "outputId": "6b1c72e4-de56-4c88-9d12-8a85b3c60ad2"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./taylor-swift-all-lyrics-30-albums\" (use force=True to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "import pdb\n",
        "\n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "file = unidecode.unidecode(open('/content/taylor-swift-all-lyrics-30-albums/data/Albums/SpeakNow_TaylorsVersion_/NeverGrowUp_TaylorsVersion_.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt6taPdpVyLk",
        "outputId": "9c0d6e7d-89a4-4b9f-afba-90f3d259de70"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file_len = 2635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_len = 200\n",
        "\n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "\n",
        "print(random_chunk())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adRpVkEvWWRB",
        "outputId": "0190d135-2456-4536-e9e1-0eb234f6811c"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "me off\n",
            "It's so much colder than I thought it would be\n",
            "So I tuck myself in and turn my nightlight on\n",
            "[Chorus]\n",
            "Wish I'd never grown up\n",
            "Wish I'd never grown up\n",
            "Oh, I don't wanna grow up\n",
            "Wish I'd never gro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "n_epochs = 5000\n",
        "print_every = 200\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 3\n",
        "lr = 0.001\n",
        "\n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0"
      ],
      "metadata": {
        "id": "958XR2wlWgEE"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 2000\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())\n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgzY26BqWrTF",
        "outputId": "31076a4a-656c-4b11-9ad5-6007a894c210"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[75.14890360832214 (200 10%) 253.6933]\n",
            "Whorlithtrit cout th nover thing wnn your I thild retled you moned likw thed day this hing'r ting you  \n",
            "\n",
            "[128.45042324066162 (400 20%) 93.6432]\n",
            "Whand I tuck moshitle Nevery honctures in your samy finger\n",
            "And ever than ve burteen, thoo\n",
            "And never gr \n",
            "\n",
            "[182.25535702705383 (600 30%) 49.3231]\n",
            "Whad gets hand's wrapped around don't lose the way to the movies\n",
            "And you're ever the burned you\n",
            "Won't  \n",
            "\n",
            "[235.3605306148529 (800 40%) 19.3677]\n",
            "Whald gets homeday and call your own shots\n",
            "You might also like[Pre-Chorus]\n",
            "But don't make her drop you \n",
            "\n",
            "[288.7493121623993 (1000 50%) 17.5575]\n",
            "Wh lotting ready for se Rcat in the world tonight\n",
            "Your little eyelidg brite songe\n",
            "So I tuck you in, tu \n",
            "\n",
            "[341.9437322616577 (1200 60%) 13.2269]\n",
            "Whats\n",
            "In a grow up\n",
            "\n",
            "[Verse 2]\n",
            "You're in the car, on the way to the mom's dropped me off\n",
            "It's so much c \n",
            "\n",
            "[395.72551441192627 (1400 70%) 14.3039]\n",
            "What\n",
            "\n",
            "[Chorus]\n",
            "Oh, darling, don't you ever grow up\n",
            "Don't you ever grow up, it could stay this simple\n",
            "I \n",
            "\n",
            "[449.1322319507599 (1600 80%) 10.7446]\n",
            "What\n",
            "And all your little brother's favorite songs\n",
            "I just realized everything I have is, someday, gonna \n",
            "\n",
            "[503.12554383277893 (1800 90%) 9.5232]\n",
            "Whar movies\n",
            "And you're mortified your mom's droppin' you off\n",
            "At fourteen, there's just so much you can \n",
            "\n",
            "[556.9739577770233 (2000 100%) 11.6684]\n",
            "Wh, someday, gonna be gone\n",
            "\n",
            "[Verse 3]\n",
            "So, here I am in my new apartment\n",
            "In a big city, they just dropp \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  start_strings = [\" to\", \" da\", \" ne\", \"I \", \" do\", \" u\", \" ju\", \" mo\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi4a1Wd2ZyYS",
        "outputId": "74f1a7bf-cfe6-459f-c6b9-5f03b4058342"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " to\n",
            " to never grow up\n",
            "\n",
            "[Post-Chorus]\n",
            "And never grow up\n",
            "\n",
            "[Verse 2]\n",
            "You're in the car, on your favourite nightlight\n",
            "\n",
            "[Pre-Chorus]\n",
            "To you, everything's funny\n",
            "You got nothing to regret\n",
            "I'd give all I have, honey \n",
            "\n",
            "I \n",
            "I don't lose the way that you dance around\n",
            "In your PJs getting ready for school\n",
            "\n",
            "[Chorus]\n",
            "Oh, darling, don't you ever grow up\n",
            "Don't you ever grow up, it could stay this simple\n",
            "I won't let nobody hurt yo \n",
            "\n",
            " ju\n",
            " just realized everything I have is, someday, gonna grow up\n",
            "Wish I'd never grow up)\n",
            "Just never grow up\n",
            "Oh (Never grow up)\n",
            "Just never grow up\n",
            "Oh (Never grow up)\n",
            "Just never grow up\n",
            "Oh (Never grow up)\n",
            "Just  \n",
            "\n",
            " do\n",
            " don't lose the way that you dance around\n",
            "In your PJs getting ready for school\n",
            "\n",
            "[Chorus]\n",
            "Oh, darling, don't you ever grow up\n",
            "Don't you ever grow up, it could stay this simple\n",
            "I won't let nobody hurt you\n",
            " \n",
            "\n",
            " ne\n",
            " never grow up\n",
            "\n",
            "[Outro]\n",
            "Oh, oh (Never grow up)\n",
            "Just never grow up, it could stay this simple\n",
            "I won't let nobody hurt you\n",
            "Won't let no one break your heart\n",
            "And even though you want to\n",
            "Plea want to\n",
            "Please, \n",
            "\n",
            " ju\n",
            " just realized everything I have is, someday, gonna be gone\n",
            "\n",
            "[Verse 3]\n",
            "So, here I am in my new apartment\n",
            "In a big city, they just dropped me off\n",
            "It's so much colder than I thought it would be\n",
            "So I tuck m \n",
            "\n",
            " ne\n",
            " never grow up\n",
            "\n",
            "[Bridge]\n",
            "Take pictures in your mind of your childhood room\n",
            "Memorize what it sounded like when your dad gets home\n",
            "Remember the footsteps, remember the words said\n",
            "And all your little brothe \n",
            "\n",
            " ne\n",
            " never grow up\n",
            "\n",
            "[Outro]\n",
            "Oh, oh (Never grow up)\n",
            "Won't let no one break your heart\n",
            "And even though you want to\n",
            "Just try to never grow up\n",
            "\n",
            "[Outro]\n",
            "Oh, oh (Never grow up)\n",
            "Won't let no one break your heart\n",
            "An \n",
            "\n",
            " u\n",
            " up\n",
            "Wish I'd never grown up, it could still be simple\n",
            "Oh, darling, don't you ever grow up\n",
            "Don't you ever grow up, just stay this little\n",
            "Oh, darling, don't you ever grow up\n",
            "Don't you ever grow up, it cou \n",
            "\n",
            " do\n",
            " don't lose the way that you dance around\n",
            "In your PJs getting ready for school\n",
            "\n",
            "[Chorus]\n",
            "Oh, darling, don't you ever grow up\n",
            "Don't you ever grow up, just stay this little\n",
            "Oh, darling, don't you ever grow \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model was able to give words but not coherent sentences. The sentences are more coherent using the Taylor Swift song, but it is probably because it has more repeated phrases, and less complicated words."
      ],
      "metadata": {
        "id": "99tIU7jAqL9m"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}